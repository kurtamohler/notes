{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a3e426",
   "metadata": {},
   "source": [
    "There are three main arguments to `torch.nn.functional.embedding_bag`. In the most basic case, they are:\n",
    "\n",
    "* `weight` is a list of vectors called \"word vectors\". In an application, each vector has a different word associated with it, but that actual word string is not needed for the operation. The word vectors all have the same number of elements.\n",
    "* `input` is a list of indices into the first dimension of `weight`. In other words, it's a list of indices of different word vectors. This list can have duplicates and it can have any length.\n",
    "* `offsets` specifies different groups, called \"bags\", of the words specified in `input`. Each successive element of `offsets` represents one bag, so each bag has one element in `offsets`. The element value is the index into `input` for the first word in the bag.\n",
    "\n",
    "For each bag specified by the combination of `offsets` and `input`, the word vectors of each word in the bag are reduced together (by mean, sum, etc.), and the return value gives the reduced vector for each bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14482d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.0500, 0.2500],\n",
      "        [0.3333, 0.4667, 0.4000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# vocab of 5 words, embedding dim = 3\n",
    "weight = torch.tensor([\n",
    "    [0.1, 0.2, 0.3],  # word 0\n",
    "    [0.0, 0.1, 0.0],  # word 1\n",
    "    [0.4, 0.0, 0.5],  # word 2\n",
    "    [0.2, 0.3, 0.1],  # word 3\n",
    "    [0.7, 0.9, 0.8],  # word 4\n",
    "    [0, 0, 0],  # dummy word\n",
    "])\n",
    "\n",
    "# 1D input indices for 2 bags: [1,2] and [0,3,4]\n",
    "input = torch.tensor([1, 2, 0, 3, 4])\n",
    "offsets = torch.tensor([0, 2])  # bag 1 starts at index 0 of `input`, bag 2 at index 2 of `input`\n",
    "\n",
    "out = F.embedding_bag(input, weight, offsets, mode='mean')\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25e1e4",
   "metadata": {},
   "source": [
    "In the above example, we create two bags with mean reduction. The first bag contains word 1 and 2, so the first vector in the output is the mean of the word vectors for words 1 and 2. Those word vectors are `[0.0, 0.1, 0.0]` and `[0.4, 0.0, 0.5]`, so the mean of those two word vectors is `[0.0 + 0.4, 0.1 + 0.0, 0.0 + 0.5] / 2 = [0.2, 0.05, 0.25]`. And the second bag contains words 0, 3, 4, and the mean of those three words is calculated for the second vector of the output.\n",
    "\n",
    "Below is a simple python implementation of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a15f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.0500, 0.2500],\n",
       "        [0.3333, 0.4667, 0.4000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_embedding_bag(input, weight, offsets, mode):\n",
    "    if mode == 'mean':\n",
    "        reduction_op = torch.mean\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    output = torch.empty(len(offsets), weight.shape[1])\n",
    "\n",
    "    for bag_idx in range(len(offsets)):\n",
    "        start = offsets[bag_idx].item()\n",
    "        end = offsets[bag_idx + 1].item() if (bag_idx + 1) < offsets.numel() else None\n",
    "        output[bag_idx] = reduction_op(weight[input[start:end]], dim=0)\n",
    "\n",
    "    return output\n",
    "\n",
    "my_embedding_bag(input, weight, offsets, mode='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec28157",
   "metadata": {},
   "source": [
    "However, this is only one way to call the operation. It is also possible to give a tensor with more than 1 dimension as the `input`, in which case `offsets` is not used, and instead the bags are specified by the leading dimension groupings of `input`. In that case, if the user wants to put different numbers of words in each bag, they can use a sentinel \"padding index\". For instance, in the following example, we create the same exact bags from the previous examples with a 2D input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1108d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.0500, 0.2500],\n",
      "        [0.3333, 0.4667, 0.4000]])\n"
     ]
    }
   ],
   "source": [
    "# 2D input indices for 2 bags: [1,2] and [0,3,4]\n",
    "input = torch.tensor([[1, 2, 5], [0, 3, 4]])\n",
    "out = F.embedding_bag(input, weight, mode='mean', padding_idx=5)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcd8cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
