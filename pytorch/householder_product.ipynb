{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a51bc8d",
   "metadata": {},
   "source": [
    "## Understanding `linalg.householder_product`\n",
    "\n",
    "The documentation of [`torch.linalg.householder_product`](https://docs.pytorch.org/docs/2.8/generated/torch.linalg.householder_product.html) describes what the operation does. I'd like to understand it and write a python implementation. I'm going to disect the description by quoting each sentence and expanding on it with a specific example scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ad124",
   "metadata": {},
   "source": [
    "> Let $\\Bbb K$ be $\\Bbb R$ or $\\Bbb C$, and let $A \\in \\Bbb K^{m\\times n}$ be a matrix with columns $a_i \\in \\Bbb K^m$ for $i = 1, ..., m$ with $m \\ge n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915befb",
   "metadata": {},
   "source": [
    "Input $A$ is a real or complex matrix with $m$ rows and $n$ columns.\n",
    "\n",
    "The matrix must have either and equal number of rows and columns or more rows than columns, $m \\ge n$.\n",
    "\n",
    "We will refer to each column of the matrix as a vector $a_i$. Since the matrix has $m$ rows, each column $a_i$ has $m$ elements.\n",
    "\n",
    "For instance, given the following matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40cc5c5",
   "metadata": {},
   "source": [
    "$$\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "A_{1 1} & A_{1 2} & A_{1 3} \\\\\n",
    "A_{2 1} & A_{2 2} & A_{2 3} \\\\\n",
    "A_{3 1} & A_{3 2} & A_{3 3} \\\\\n",
    "A_{4 1} & A_{4 2} & A_{4 3} \\\\\n",
    "A_{5 1} & A_{5 2} & A_{5 3} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f527ff",
   "metadata": {},
   "source": [
    "We have 5 rows and 3 columns, so $m = 5$ and $n = 3$. $m \\ge n$ is satisfied.\n",
    "\n",
    "Each $a_i$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3acbb71",
   "metadata": {},
   "source": [
    "$$\n",
    "a_1 = \\begin{pmatrix}\n",
    "A_{1 1} \\\\\n",
    "A_{2 1} \\\\\n",
    "A_{3 1} \\\\\n",
    "A_{4 1} \\\\\n",
    "A_{5 1}\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\quad\n",
    "\n",
    "a_2 = \\begin{pmatrix}\n",
    "A_{1 2} \\\\\n",
    "A_{2 2} \\\\\n",
    "A_{3 2} \\\\\n",
    "A_{4 2} \\\\\n",
    "A_{5 2}\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\quad\n",
    "\n",
    "a_3 = \\begin{pmatrix}\n",
    "A_{1 3} \\\\\n",
    "A_{2 3} \\\\\n",
    "A_{3 3} \\\\\n",
    "A_{4 3} \\\\\n",
    "A_{5 3}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbfaae",
   "metadata": {},
   "source": [
    "> Denote by $b_i$ the vector resulting from zeroing out the first $i - 1$ components of $a_i$ and setting to $1$ the $i$-th."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60d236",
   "metadata": {},
   "source": [
    "In our case:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783edd03",
   "metadata": {},
   "source": [
    "$$\n",
    "b_1 = \\begin{pmatrix}\n",
    "1 \\\\\n",
    "A_{2 1} \\\\\n",
    "A_{3 1} \\\\\n",
    "A_{4 1} \\\\\n",
    "A_{5 1}\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\quad\n",
    "\n",
    "b_2 = \\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "A_{3 2} \\\\\n",
    "A_{4 2} \\\\\n",
    "A_{5 2}\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\quad\n",
    "\n",
    "b_3 = \\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "A_{4 3} \\\\\n",
    "A_{5 3}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2276ff8",
   "metadata": {},
   "source": [
    "> For a vector $\\tau \\in \\Bbb K^k$ with $k \\le n$, this function computes the first $n$ columns of the matrix\n",
    ">\n",
    ">  $H_1 H_2 ... H_k$ with $H_i = I_m - \\tau_i b_i b_i^H$\n",
    ">\n",
    "> where $I_m$ is the m-dimensional identity matrix and $b^H$ is the conjugate transpose when $b$ is complex and the transpose when $b$ is real-valued."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd4676",
   "metadata": {},
   "source": [
    "So input $\\tau$ is a vector that must have length equal to or less than the number of columns in $A$, $k \\le n$. (In the formula for the product of $H_i$ matrices, if $k \\lt n$ then the out of bounds values of $\\tau_i$ are taken to be 0, so that the corresponding $H_i$'s are the identity matrix.)\n",
    "\n",
    "In our case, let's use the following $\\tau$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577beb29",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tau = \\begin{pmatrix}\n",
    "\\tau_1 \\\\\n",
    "\\tau_2 \\\\\n",
    "\\tau_3\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ee187",
   "metadata": {},
   "source": [
    "One thing that isn't 100% clear to me is which kind of vector product to use for the $b_i b_i^H$ term. From the [Wikipedia article](https://en.wikipedia.org/wiki/Householder_transformation#:~:text=.-,Householder%20matrix,-%5Bedit%5D) on the Householder transformation, the formula for a Householder matrix supposedly uses the outer product for this term. So I'll go ahead and assume that must be what we need to use here as well.\n",
    "\n",
    "The $b^H$ vectors are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edae1f",
   "metadata": {},
   "source": [
    "$$\n",
    "b_1^H = \\begin{pmatrix}\n",
    "1 &\n",
    "A_{2 1}^* &\n",
    "A_{3 1}^* &\n",
    "A_{4 1}^* &\n",
    "A_{5 1}^*\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "b_2^H = \\begin{pmatrix}\n",
    "0 &\n",
    "1 &\n",
    "A_{3 2}^* &\n",
    "A_{4 2}^* &\n",
    "A_{5 2}^*\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "b_3^H = \\begin{pmatrix}\n",
    "0 &\n",
    "0 &\n",
    "1 &\n",
    "A_{4 3}^* &\n",
    "A_{5 3}^*\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb55f0",
   "metadata": {},
   "source": [
    "Then the outer products are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75abe01",
   "metadata": {},
   "source": [
    "$$\n",
    "b_1 b_1^H = \\begin{pmatrix}\n",
    "1 & A_{2 1}^* & A_{3 1}^* & A_{4 1}^* & A_{5 1}^* \\\\\n",
    "A_{2 1} & A_{2 1} A_{2 1}^* & A_{2 1} A_{3 1}^* & A_{2 1} A_{4 1}^* & A_{2 1} A_{5 1}^* \\\\\n",
    "A_{3 1} & A_{3 1} A_{2 1}^* & A_{3 1} A_{3 1}^* & A_{3 1} A_{4 1}^* & A_{3 1} A_{5 1}^* \\\\\n",
    "A_{4 1} & A_{4 1} A_{2 1}^* & A_{4 1} A_{3 1}^* & A_{4 1} A_{4 1}^* & A_{4 1} A_{5 1}^* \\\\\n",
    "A_{5 1} & A_{5 1} A_{2 1}^* & A_{5 1} A_{3 1}^* & A_{5 1} A_{4 1}^* & A_{5 1} A_{5 1}^* \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e7d86",
   "metadata": {},
   "source": [
    "$$\n",
    "b_2 b_2^H = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & A_{3 2}^* & A_{4 2}^* & A_{5 2}^* \\\\\n",
    "0 & A_{3 2} & A_{3 2} A_{3 2}^* & A_{3 2} A_{4 2}^* & A_{3 2} A_{5 2}^* \\\\\n",
    "0 & A_{4 2} & A_{4 2} A_{3 2}^* & A_{4 2} A_{4 2}^* & A_{4 2} A_{5 2}^* \\\\\n",
    "0 & A_{5 2} & A_{5 2} A_{3 2}^* & A_{5 2} A_{4 2}^* & A_{5 2} A_{5 2}^* \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51dfb96",
   "metadata": {},
   "source": [
    "$$\n",
    "b_3 b_3^H = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & A_{4 3}^* & A_{5 3}^* \\\\\n",
    "0 & 0 & A_{4 3} & A_{4 3} A_{4 3}^* & A_{4 3} A_{5 3}^* \\\\\n",
    "0 & 0 & A_{5 3} & A_{5 3} A_{4 3}^* & A_{5 3} A_{5 3}^* \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a365d",
   "metadata": {},
   "source": [
    "## Python prototype implementation\n",
    "\n",
    "I'll implement it in python now to make sure I understand it, and I'll compare its output to that of the official pytorch impl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7bb1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "\n",
    "def run_test(fn, fn_check):\n",
    "    dtypes = [\n",
    "        torch.double,\n",
    "        torch.cdouble,\n",
    "    ]\n",
    "\n",
    "    shapes = [\n",
    "        # [A_shape, tau_shape]\n",
    "        [(5, 3), (3,)],\n",
    "        [(5, 3), (2,)],\n",
    "        [(5, 3), (1,)],\n",
    "        [(10, 5, 3), (10, 3,)],\n",
    "        [(10, 5, 3), (10, 2,)],\n",
    "        [(2, 10, 5, 3), (2, 10, 3,)],\n",
    "        [(40, 1, 20, 15), (40, 1, 10,)],\n",
    "    ]\n",
    "\n",
    "    for dtype, (A_shape, tau_shape) in itertools.product(dtypes, shapes):\n",
    "        A = torch.randn(A_shape, dtype=dtype)\n",
    "        tau = torch.randn(tau_shape, dtype=dtype)\n",
    "\n",
    "        r_check = fn_check(A, tau)\n",
    "        r = fn(A, tau)\n",
    "        assert torch.allclose(r, r_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b5bdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def householder_prod_0(A, tau):\n",
    "    m = A.shape[-2]\n",
    "    n = A.shape[-1]\n",
    "    k = tau.shape[-1]\n",
    "\n",
    "    I = torch.eye(m, dtype=A.dtype)\n",
    "    H_prod = I\n",
    "\n",
    "    for i in range(k):\n",
    "        b = A[..., i].clone()\n",
    "        b[..., :i] = 0\n",
    "        b[..., i] = 1\n",
    "        b_bH = b.unsqueeze(-1) @ b.conj().unsqueeze(-2)\n",
    "        tau_i = tau[..., i, None, None]\n",
    "        H = I - tau_i * b_bH\n",
    "        H_prod = H_prod @ H\n",
    "\n",
    "    return H_prod[..., :n]\n",
    "\n",
    "run_test(householder_prod_0, torch.linalg.householder_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63204ec1",
   "metadata": {},
   "source": [
    "This implementation gives the same outputs as the pytorch impl for the above cases. But each of the householder matrices is calculated serially. We should be able to batch that part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b700e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def householder_prod_1(A, tau):\n",
    "    m = A.shape[-2]\n",
    "    n = A.shape[-1]\n",
    "    k = tau.shape[-1]\n",
    "\n",
    "    B = A[..., :k].transpose(-1, -2).clone()\n",
    "\n",
    "    for i in range(k):\n",
    "        B[..., i, :i] = 0\n",
    "        B[..., i, i] = 1\n",
    "\n",
    "    I = torch.eye(m, dtype=A.dtype)\n",
    "    B_BH = B.unsqueeze(-1) @ B.conj().unsqueeze(-2)\n",
    "    H_matrices = I - tau[..., None, None] * B_BH\n",
    "    H_prod = H_matrices[..., 0, :, :]\n",
    "\n",
    "    for i in range(1, k):\n",
    "        H = H_matrices[..., i, :, :]\n",
    "        H_prod = H_prod @ H\n",
    "\n",
    "    return H_prod[..., :n]\n",
    "\n",
    "run_test(householder_prod_1, torch.linalg.householder_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236292d",
   "metadata": {},
   "source": [
    "That should give better performance. We're still looping over the matrices to perform the matmul product, but we have to do that because there is no matrix product reduction operation in pytorch. We're also looping to create the `B` vectors, and there may be a better way to do that in python, but this python impl is only a prototype, which has now served its purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68275cf1",
   "metadata": {},
   "source": [
    "## Planning a Metal GPU impl\n",
    "\n",
    "I need to implement `householder_product` to run on the Apple Metal GPU. A few possible strategies occur to me:\n",
    "\n",
    "1. Use only existing `at::` operations.\n",
    "\n",
    "2. Use an existing linalg library that has Metal gpu support.\n",
    "\n",
    "3. Write a single Metal kernel to do the whole operation.\n",
    "\n",
    "4. Write a Metal kernel to do the `(A, tau) -> H_matrices` calculation, and then iteratively call the existing `matmul` op to calculate the product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326aaba1",
   "metadata": {},
   "source": [
    "Option (1) is of course very easy to do, now that I have the python impl, but it will be the least efficient one. Since it is so easy to do, I should do this one first and just check how the performance compares to the CPU impl.\n",
    "\n",
    "It seems that option (2) is not possible, because I can't find any existing implementation of orgqr for Metal.\n",
    "\n",
    "I think option (3) would probably be pretty difficult, and I think it would actually have to operate serially anyway to do the matrix product.\n",
    "\n",
    "So it seems to me that option (4) is the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e4028",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
