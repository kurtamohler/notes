{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ml-agents\n",
    "\n",
    "Unity has a toolkit called `ml-agents` which allows you to use a Unity game as a training environment: <https://github.com/Unity-Technologies/ml-agents>\n",
    "\n",
    "Documentation: <https://unity-technologies.github.io/ml-agents/>\n",
    "\n",
    "My task is to add an environment wrapper class to torchrl that will call into the environment for `ml-agents`.\n",
    "\n",
    "## How to use ml-agents\n",
    "\n",
    "Installation instructions: <https://unity-technologies.github.io/ml-agents/Installation/>\n",
    "\n",
    "Install the python interface: <https://unity-technologies.github.io/ml-agents/ML-Agents-Envs-README/>\n",
    "\n",
    "Getting started guide: <https://unity-technologies.github.io/ml-agents/Getting-Started/>\n",
    "\n",
    "`ml-agents` currently has a different interfaces into the environments, and we'll need to use one of them:\n",
    "* gym: <https://unity-technologies.github.io/ml-agents/Python-Gym-API/>\n",
    "  - This doesn't support multi-agent, so probably not the best option\n",
    "* pettingzoo: <https://unity-technologies.github.io/ml-agents/Python-PettingZoo-API/>\n",
    "* native API: <https://unity-technologies.github.io/ml-agents/Python-LLAPI/>\n",
    "\n",
    "\n",
    "The `ml-agents` environments are defined here: <https://github.com/Unity-Technologies/ml-agents/tree/develop/ml-agents-envs/mlagents_envs>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every agent in MLAgents has an associated behavior spec. A number of agents can share the same behavior spec, but all agents could have a different behavior spec as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3DBall\n",
    "\n",
    "```\n",
    "behavior_specs:\n",
    "  behavior_name: 3DBall?team=0\n",
    "    observation_spec[0]:\n",
    "      name: VectorSensor_size8\n",
    "      shape: (8,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 2, Discrete: ()\n",
    "      is_continuous: True\n",
    "      continuous_size: 2\n",
    "      is_discrete: False\n",
    "      discrete_branches: ()\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: []\n",
    "      continuous: [[0.47754392 0.9049918 ]]\n",
    "current steps:\n",
    "  behavior_name: 3DBall?team=0\n",
    "    decision step:\n",
    "      agent_id: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
    "      group_id: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "      observation_shapes: [(12, 8)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask: None\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 8)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StrikersVsGoalie\n",
    "\n",
    "```\n",
    "behavior_specs:\n",
    "  behavior_name: Striker?team=0\n",
    "    observation_spec[0]:\n",
    "      name: StackingSensor_size3_BlueRayPerceptionSensor\n",
    "      shape: (231,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[1]:\n",
    "      name: StackingSensor_size3_BlueRayPerceptionSensorReverse\n",
    "      shape: (63,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 0, Discrete: (3, 3, 3)\n",
    "      is_continuous: False\n",
    "      continuous_size: 0\n",
    "      is_discrete: True\n",
    "      discrete_branches: (3, 3, 3)\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: [[2 0 0]]\n",
    "      continuous: []\n",
    "  behavior_name: Goalie?team=1\n",
    "    observation_spec[0]:\n",
    "      name: StackingSensor_size3_PurpleGoalieRayPerceptionSensor\n",
    "      shape: (738,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 0, Discrete: (3, 3, 3)\n",
    "      is_continuous: False\n",
    "      continuous_size: 0\n",
    "      is_discrete: True\n",
    "      discrete_branches: (3, 3, 3)\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: [[2 1 2]]\n",
    "      continuous: []\n",
    "current steps:\n",
    "  behavior_name: Striker?team=0\n",
    "    decision step:\n",
    "      agent_id: [ 0  2  3  5  6  8  9 11 12 14 15 17 18 20 21 23]\n",
    "      group_id: [1, 1, 3, 3, 5, 5, 7, 7, 9, 9, 11, 11, 13, 13, 15, 15]\n",
    "      observation_shapes: [(16, 231), (16, 63)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask_shapes: [(16, 3), (16, 3), (16, 3)]\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 231), (0, 63)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "  behavior_name: Goalie?team=1\n",
    "    decision step:\n",
    "      agent_id: [ 1  4  7 10 13 16 19 22]\n",
    "      group_id: [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "      observation_shapes: [(8, 738)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask_shapes: [(8, 3), (8, 3), (8, 3)]\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 738)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoccerTwos\n",
    "\n",
    "```\n",
    "behavior_specs:\n",
    "  behavior_name: SoccerTwos?team=1\n",
    "    observation_spec[0]:\n",
    "      name: StackingSensor_size3_PurpleRayPerceptionSensor\n",
    "      shape: (264,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[1]:\n",
    "      name: StackingSensor_size3_PurpleRayPerceptionSensorReverse\n",
    "      shape: (72,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 0, Discrete: (3, 3, 3)\n",
    "      is_continuous: False\n",
    "      continuous_size: 0\n",
    "      is_discrete: True\n",
    "      discrete_branches: (3, 3, 3)\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: [[0 1 1]]\n",
    "      continuous: []\n",
    "  behavior_name: SoccerTwos?team=0\n",
    "    observation_spec[0]:\n",
    "      name: StackingSensor_size3_BlueRayPerceptionSensor\n",
    "      shape: (264,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[1]:\n",
    "      name: StackingSensor_size3_BlueRayPerceptionSensorReverse\n",
    "      shape: (72,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 0, Discrete: (3, 3, 3)\n",
    "      is_continuous: False\n",
    "      continuous_size: 0\n",
    "      is_discrete: True\n",
    "      discrete_branches: (3, 3, 3)\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: [[1 2 1]]\n",
    "      continuous: []\n",
    "current steps:\n",
    "  behavior_name: SoccerTwos?team=1\n",
    "    decision step:\n",
    "      agent_id: [ 0  2  4  6  8 10 12 14 16 18 20 22 24 26 28 30]\n",
    "      group_id: [2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14, 16, 16]\n",
    "      observation_shapes: [(16, 264), (16, 72)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask_shapes: [(16, 3), (16, 3), (16, 3)]\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 264), (0, 72)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "  behavior_name: SoccerTwos?team=0\n",
    "    decision step:\n",
    "      agent_id: [ 1  3  5  7  9 11 13 15 17 19 21 23 25 27 29 31]\n",
    "      group_id: [1, 1, 3, 3, 5, 5, 7, 7, 9, 9, 11, 11, 13, 13, 15, 15]\n",
    "      observation_shapes: [(16, 264), (16, 72)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask_shapes: [(16, 3), (16, 3), (16, 3)]\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 264), (0, 72)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walker\n",
    "\n",
    "```\n",
    "behavior_specs:\n",
    "  behavior_name: Walker?team=0\n",
    "    observation_spec[0]:\n",
    "      name: VectorSensor_size243\n",
    "      shape: (243,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 39, Discrete: ()\n",
    "      is_continuous: True\n",
    "      continuous_size: 39\n",
    "      is_discrete: False\n",
    "      discrete_branches: ()\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: []\n",
    "      continuous: [[ 0.8072419   0.7803087   0.01151393  0.20298165  0.9033102  -0.65760577\n",
    "   0.5174855  -0.588782   -0.03862659 -0.0283867  -0.5692337   0.8303417\n",
    "  -0.06972238 -0.49072945  0.90130824 -0.8372041  -0.7015494  -0.7628687\n",
    "  -0.72954524  0.3241689   0.32257587 -0.60499096 -0.9076488  -0.64096147\n",
    "   0.17179826  0.6753697  -0.64479506  0.2613258  -0.83917886  0.8475276\n",
    "  -0.939543    0.85671264  0.18680607 -0.29868662  0.24222809  0.09251133\n",
    "   0.03180926 -0.5663932  -0.556755  ]]\n",
    "current steps:\n",
    "  behavior_name: Walker?team=0\n",
    "    decision step:\n",
    "      agent_id: [0 1 2 3 4 5 6 7 8 9]\n",
    "      group_id: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "      observation_shapes: [(10, 243)]\n",
    "      reward: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
    " 0.0000000e+00 5.1639726e-14 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask: None\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 243)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallway\n",
    "\n",
    "```\n",
    "behavior_specs:\n",
    "  behavior_name: Hallway?team=0\n",
    "    observation_spec[0]:\n",
    "      name: StackingSensor_size3_RayPerceptionSensor\n",
    "      shape: (105,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[1]:\n",
    "      name: StackingSensor_size3_VectorSensor_size1\n",
    "      shape: (3,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 0, Discrete: (5,)\n",
    "      is_continuous: False\n",
    "      continuous_size: 0\n",
    "      is_discrete: True\n",
    "      discrete_branches: (5,)\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: [[1]]\n",
    "      continuous: []\n",
    "current steps:\n",
    "  behavior_name: Hallway?team=0\n",
    "    decision step:\n",
    "      agent_id: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
    "      group_id: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "      observation_shapes: [(16, 105), (16, 3)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask_shapes: [(16, 5)]\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 105), (0, 3)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyramids\n",
    "\n",
    "```\n",
    "behavior_specs:\n",
    "  behavior_name: Pyramids?team=0\n",
    "    observation_spec[0]:\n",
    "      name: RayPerceptionSensor\n",
    "      shape: (56,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[1]:\n",
    "      name: RayPerceptionSensor1\n",
    "      shape: (56,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[2]:\n",
    "      name: RayPerceptionSensor2\n",
    "      shape: (56,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[3]:\n",
    "      name: VectorSensor_size4\n",
    "      shape: (4,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 0, Discrete: (5,)\n",
    "      is_continuous: False\n",
    "      continuous_size: 0\n",
    "      is_discrete: True\n",
    "      discrete_branches: (5,)\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: [[3]]\n",
    "      continuous: []\n",
    "current steps:\n",
    "  behavior_name: Pyramids?team=0\n",
    "    decision step:\n",
    "      agent_id: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
    "      group_id: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "      observation_shapes: [(16, 56), (16, 56), (16, 56), (16, 4)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask_shapes: [(16, 5)]\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 56), (0, 56), (0, 56), (0, 4)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DungeonEscape\n",
    "\n",
    "```\n",
    "behavior_specs:\n",
    "  behavior_name: DungeonEscape?team=0\n",
    "    observation_spec[0]:\n",
    "      name: RBSensor\n",
    "      shape: (10,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[1]:\n",
    "      name: StackingSensor_size3_RayPerceptionSensor\n",
    "      shape: (360,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[2]:\n",
    "      name: VectorSensor_size1\n",
    "      shape: (1,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 0, Discrete: (7,)\n",
    "      is_continuous: False\n",
    "      continuous_size: 0\n",
    "      is_discrete: True\n",
    "      discrete_branches: (7,)\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: [[0]]\n",
    "      continuous: []\n",
    "current steps:\n",
    "  behavior_name: DungeonEscape?team=0\n",
    "    decision step:\n",
    "      agent_id: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
    " 24 25 26 27 28 29 30 31 32 33 34 35]\n",
    "      group_id: [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10, 11, 11, 11, 12, 12, 12]\n",
    "      observation_shapes: [(36, 10), (36, 360), (36, 1)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask_shapes: [(36, 7)]\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 10), (0, 360), (0, 1)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridFoodCollector\n",
    "\n",
    "```\n",
    "behavior_specs:\n",
    "  behavior_name: GridFoodCollector?team=0\n",
    "    observation_spec[0]:\n",
    "      name: GridSensor-OneHot\n",
    "      shape: (5, 40, 40)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 3, Discrete: (2,)\n",
    "      is_continuous: False\n",
    "      continuous_size: 3\n",
    "      is_discrete: False\n",
    "      discrete_branches: (2,)\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: [[1]]\n",
    "      continuous: [[0.04317868 0.8829177  0.15289028]]\n",
    "      continuous.dtype: float32\n",
    "current steps:\n",
    "  behavior_name: GridFoodCollector?team=0\n",
    "    decision step:\n",
    "      agent_id: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
    "      group_id: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "      observation_shapes: [(20, 5, 40, 40)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask_shapes: [(20, 2)]\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 5, 40, 40)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WallJump\n",
    "\n",
    "```\n",
    "behavior_specs:\n",
    "  behavior_name: SmallWallJump?team=0\n",
    "    observation_spec[0]:\n",
    "      name: StackingSensor_size6_OffsetRayPerceptionSensor\n",
    "      shape: (210,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[1]:\n",
    "      name: StackingSensor_size6_RayPerceptionSensor\n",
    "      shape: (210,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[2]:\n",
    "      name: StackingSensor_size6_VectorSensor_size4\n",
    "      shape: (24,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 0, Discrete: (3, 3, 3, 2)\n",
    "      is_continuous: False\n",
    "      continuous_size: 0\n",
    "      is_discrete: True\n",
    "      discrete_branches: (3, 3, 3, 2)\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: [[0 2 1 1]]\n",
    "      continuous: []\n",
    "      continuous.dtype: float32\n",
    "  behavior_name: BigWallJump?team=0\n",
    "    observation_spec[0]:\n",
    "      name: StackingSensor_size6_OffsetRayPerceptionSensor\n",
    "      shape: (210,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[1]:\n",
    "      name: StackingSensor_size6_RayPerceptionSensor\n",
    "      shape: (210,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    observation_spec[2]:\n",
    "      name: StackingSensor_size6_VectorSensor_size4\n",
    "      shape: (24,)\n",
    "      dimension_property: (<DimensionProperty.NONE: 1>,)\n",
    "      observation_type: ObservationType.DEFAULT\n",
    "    action_spec: Continuous: 0, Discrete: (3, 3, 3, 2)\n",
    "      is_continuous: False\n",
    "      continuous_size: 0\n",
    "      is_discrete: True\n",
    "      discrete_branches: (3, 3, 3, 2)\n",
    "    random_action:\n",
    "      discrete_dtype: <class 'numpy.int32'>\n",
    "      discrete: [[0 0 1 1]]\n",
    "      continuous: []\n",
    "      continuous.dtype: float32\n",
    "current steps:\n",
    "  behavior_name: SmallWallJump?team=0\n",
    "    decision step:\n",
    "      agent_id: [ 0  1  4  5  7 11 12 13 15 17 20 22]\n",
    "      group_id: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "      observation_shapes: [(12, 210), (12, 210), (12, 24)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask_shapes: [(12, 3), (12, 3), (12, 3), (12, 2)]\n",
    "    terminal step:\n",
    "      agent_id: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
    "      group_id: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "      observation_shapes: [(24, 210), (24, 210), (24, 24)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "  behavior_name: BigWallJump?team=0\n",
    "    decision step:\n",
    "      agent_id: [ 2  3  6  8  9 10 14 16 18 19 21 23]\n",
    "      group_id: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "      observation_shapes: [(12, 210), (12, 210), (12, 24)]\n",
    "      reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      group_reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "      action_mask_shapes: [(12, 3), (12, 3), (12, 3), (12, 2)]\n",
    "    terminal step:\n",
    "      agent_id: []\n",
    "      group_id: []\n",
    "      observation_shapes: [(0, 210), (0, 210), (0, 24)]\n",
    "      reward: []\n",
    "      group_reward: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New agents\n",
    "\n",
    "DungeonEscape and Basic add new agents during a run! This needs to be supported somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New observation? Agent ID reassigned?\n",
    "\n",
    "Running the `TestUnityMLAgents.test_env_with_editor` test with DungeonEscape gives this error:\n",
    "\n",
    "```\n",
    "  File \"/home/endoplasm/develop/torchrl-mlagents/torchrl/envs/libs/unity_mlagents.py\", line 301, in _make_td_out\n",
    "    agent_dict[obs_name] = tensordict_in[group_name, agent_name, obs_name]\n",
    "...\n",
    "KeyError: 'key \"RBSensor\" not found in TensorDict with keys [\\'discrete_action\\']'\n",
    "```\n",
    "\n",
    "Either RBSensor is a new observation that got added to the behavior spec, or the agent ID in question was reassigned to a different agent ID with a different behavior than it had before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
