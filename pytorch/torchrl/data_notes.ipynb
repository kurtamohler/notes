{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are my notes on the [Data Collection and Storage tutorial](https://pytorch.org/rl/stable/tutorials/getting-started-3.html) in TorchRL, plus some notes at the end about how replay buffers currently interact with `torch.compile`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collectors\n",
    "\n",
    "TorchRL has the concept of a data collector, which has the following responsibilities:\n",
    "\n",
    "* Execute whatever policy you give it within any environment you give it.\n",
    "* Reset the environment when necessary, e.g. upon termination. Does not automatically reset between consecutive batches of data, unlike `rollout()`.\n",
    "* Collect batches of data of a predefined size.\n",
    "* The data collected contains the actions generated from the policy and the states of the environment at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collector needs to be given the arguments:\n",
    "\n",
    "* The size of each batch to collect, called `frames_per_batch`.\n",
    "* The total number of frames to iterate over, called `total_frames`. If `total_frames=-1`, then the number of frames is infinite.\n",
    "* A policy.\n",
    "* An environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchrl\n",
    "\n",
    "torch.manual_seed(0)\n",
    "env = torchrl.envs.GymEnv(\"CartPole-v1\")\n",
    "env.set_seed(0)\n",
    "policy = torchrl.envs.utils.RandomPolicy(env.action_spec)\n",
    "collector = torchrl.collectors.SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    frames_per_batch=200,\n",
    "    total_frames=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([200, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([200]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([200]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([200, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([200]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([200, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([200]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "for data in collector:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are guaranteed to get a batch of size 200, and a single trajectory may be shorter than that, the collected data contains ID numbers for each individual trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "print(data['collector', 'traj_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffers\n",
    "\n",
    "After collecting data, we need to store it. Typically, the data is only stored temporarily and then cleared according to some heuristic. TorchRL provides replay buffers for this task. The following aspects of a replay buffer can be specified:\n",
    "\n",
    "* The storage type.\n",
    "* The sampling technique.\n",
    "* The writing heuristic.\n",
    "* Transforms to apply to the data.\n",
    "\n",
    "A generic replay buffer just needs to know the storage type. There are several storage types in TorchRL, and they usually need to be given the maximum size of the data that they will hold. We'll use `LazyMemmapStorage`, which is a type of storage which has two useful features:\n",
    "\n",
    "* It lazily updates so that you don't need to tell it ahead of time what kind of data it will need to hold.\n",
    "* It uses `MemoryMappedTensor` so that tensors added to it are memory mapped, which allows them to be saved to disk efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = torchrl.data.replay_buffers.ReplayBuffer(\n",
    "    storage=torchrl.data.replay_buffers.LazyMemmapStorage(max_size=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add data to the buffer with either `add()`, to add a single element, or `extend()` to add multiple elements. Since the data we collected earlier has 200 elements, we'll use `extend()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = buffer.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(buffer))\n",
    "assert len(buffer) == collector.frames_per_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take samples from the replay buffer, we can use the `sample()` method. Since we didn't specify any sampling technique, it is purely random, meaning that uniqueness is not guaranteed. Let's take a sample of 30 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([30, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([30]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([30]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([30, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([30]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([30, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([30]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "tensor([7, 2, 2, 9, 4, 6, 4, 6, 4, 1, 4, 2, 1, 7, 9, 2, 4, 7, 6, 2, 4, 7, 7, 7,\n",
      "        4, 7, 6, 3, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "sample = buffer.sample(batch_size=30)\n",
    "print(sample)\n",
    "print(sample['collector', 'traj_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the tensordict returned by `sample()` is the same as that returned by the collector, except that we have a different batch size, and the order is random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling replay buffers\n",
    "\n",
    "At the moment, some of the samplers have an argument that enables the `sample()` method to be compiled. In particular, the `*SliceSampler` classes have this feature. However, we don't just want to compile the sampler, we would also like to compile the `extend()` method on replay buffers.\n",
    "\n",
    "The first thing do to is to support compiling `extend()` for `ReplayBuffer(storage=LazyTensorStorage(1000))`\n",
    "\n",
    "We should probably use the same interface for compiling that `SliceSampler` offers, which is a `bool` arg called `compile`. If set to `True` or given a dict of compiler args, `SliceSampler.__init__` runs `self._get_index = torch.compile(self._get_index, **kwargs)` to compile the method that does the sampling work.\n",
    "\n",
    "So we should add the same arg to `ReplayBuffer` and compile the method that does the work of extending the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = torchrl.data.ReplayBuffer(\n",
    "    storage=torchrl.data.LazyTensorStorage(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchrl.data.replay_buffers.samplers.RandomSampler'>\n",
      "<class 'torchrl.data.replay_buffers.writers.RoundRobinWriter'>\n"
     ]
    }
   ],
   "source": [
    "print(type(buffer._sampler))\n",
    "print(type(buffer._writer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompiles during test\n",
    "\n",
    "@vmoens, you mentioned offline that you've seen recompiles every time you tried to call `extend` on a `ReplayBuffer(storage=LazyTensorStorage(1000))`. Could you share with me a case where that happens? The test I added in #2504 only has recompiles on the first two calls.\n",
    "\n",
    "As for the recompiles that I have seen, if I set `num_extend_before_capture = 0` in the test, it no longer ignores those recompiles, and I get these recompile records:\n",
    "\n",
    "```\n",
    "$ python test/test_rb.py -k test_extend_recompile[100-ReplayBuffer-LazyTensorStorage-tensor-RoundRobinWriter-RandomSampler]\n",
    "...\n",
    "[11/1] [__recompiles] Recompiling function _lazy_call_fn in /home/endoplasm/develop/torchrl-0/torchrl/_utils.py:389\n",
    "[11/1] [__recompiles]     triggered by the following guard failure(s):\n",
    "[11/1] [__recompiles]     - 11/0: L['self'].func_name == 'torchrl.data.replay_buffers.storages.TensorStorage.set'\n",
    "[12/1] [__recompiles] Recompiling function torch_dynamo_resume_in__lazy_call_fn_at_394 in /home/endoplasm/develop/torchrl-0/torchrl/_utils.py:394\n",
    "[12/1] [__recompiles]     triggered by the following guard failure(s):\n",
    "[12/1] [__recompiles]     - 12/0: len(L['args']) == 3                                         \n",
    "[18/1] [__recompiles] Recompiling function torch_dynamo_resume_in_set_at_713 in /home/endoplasm/develop/torchrl-0/torchrl/data/replay_buffers/storages.py:713\n",
    "[18/1] [__recompiles]     triggered by the following guard failure(s):\n",
    "[18/1] [__recompiles]     - 18/0: ___check_obj_id(L['self'].initialized, 8907584) \n",
    "```\n",
    "\n",
    "So there are three recompiles to look into.\n",
    "\n",
    "The first and second one are caused by the use of the `implement_for` decorator [here](https://github.com/pytorch/rl/blob/baba52b9a13d5416ff622e486fee9b3f05f51f2f/torchrl/data/replay_buffers/storages.py#L686). Since this is the first time the `_lazy_call_fn` function within the `implement_for.__call__` method is being called with the string `self.func_name = \"torchrl.data.replay_buffers.storages.TensorStorage.set\"`, and since `torch.compile` has to recompile every time a function is given a different string argument, I'm not sure there is much we can/should do about this one. Maybe it would be possible to change how `implement_for` works to avoid this recompile, but maybe it's not worth the trouble since these only happen once.\n",
    "\n",
    "The third recompile is caused by the fact that [this branch of `TensorStorage.set()`](https://github.com/pytorch/rl/blob/baba52b9a13d5416ff622e486fee9b3f05f51f2f/torchrl/data/replay_buffers/storages.py#L715) is only visited in the first call. I'd guess that this also is not worth the trouble to try to fix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried changing `TensorStorage._len_value` to be just an int, rather than `mp.Value`, and that did not avoid the recompiles.\n",
    "\n",
    "```\n",
    "diff --git a/torchrl/data/replay_buffers/storages.py b/torchrl/data/replay_buffers/storages.py\n",
    "index 217229b5..145bcdd5 100644\n",
    "--- a/torchrl/data/replay_buffers/storages.py\n",
    "+++ b/torchrl/data/replay_buffers/storages.py\n",
    "@@ -149,6 +149,7 @@ class Storage:\n",
    "         if self.ndim == 1:\n",
    "             return torch.randint(\n",
    "                 0,\n",
    "+                #len(self),\n",
    "                 self._len,\n",
    "                 (batch_size,),\n",
    "                 generator=self._rng,\n",
    "@@ -458,15 +459,12 @@ class TensorStorage(Storage):\n",
    "     def _len(self):\n",
    "         _len_value = self.__dict__.get(\"_len_value\", None)\n",
    "         if _len_value is None:\n",
    "-            _len_value = self._len_value = mp.Value(\"i\", 0)\n",
    "-        return _len_value.value\n",
    "+            _len_value = self._len_value = 0\n",
    "+        return _len_value\n",
    " \n",
    "     @_len.setter\n",
    "     def _len(self, value):\n",
    "-        _len_value = self.__dict__.get(\"_len_value\", None)\n",
    "-        if _len_value is None:\n",
    "-            _len_value = self._len_value = mp.Value(\"i\", 0)\n",
    "-        _len_value.value = value\n",
    "+        self._len_value = value\n",
    " \n",
    "     @property\n",
    "     def _total_shape(self):\n",
    "@@ -606,7 +604,7 @@ class TensorStorage(Storage):\n",
    "     def __setstate__(self, state):\n",
    "         len = state.pop(\"len__context\", None)\n",
    "         if len is not None:\n",
    "-            _len_value = mp.Value(\"i\", len)\n",
    "+            _len_value = len\n",
    "             state[\"_len_value\"] = _len_value\n",
    "         self.__dict__.update(state)\n",
    "```\n",
    "\n",
    "```\n",
    "diff --git a/torchrl/data/replay_buffers/writers.py b/torchrl/data/replay_buffers/writers.py\n",
    "index 3a95c397..115e4ff0 100644\n",
    "--- a/torchrl/data/replay_buffers/writers.py\n",
    "+++ b/torchrl/data/replay_buffers/writers.py\n",
    "@@ -214,29 +214,23 @@ class RoundRobinWriter(Writer):\n",
    "     def _cursor(self):\n",
    "         _cursor_value = self.__dict__.get(\"_cursor_value\", None)\n",
    "         if _cursor_value is None:\n",
    "-            _cursor_value = self._cursor_value = mp.Value(\"i\", 0)\n",
    "-        return _cursor_value.value\n",
    "+            _cursor_value = self._cursor_value = 0\n",
    "+        return _cursor_value\n",
    " \n",
    "     @_cursor.setter\n",
    "     def _cursor(self, value):\n",
    "-        _cursor_value = self.__dict__.get(\"_cursor_value\", None)\n",
    "-        if _cursor_value is None:\n",
    "-            _cursor_value = self._cursor_value = mp.Value(\"i\", 0)\n",
    "-        _cursor_value.value = value\n",
    "+        self._cursor_value = value\n",
    " \n",
    "     @property\n",
    "     def _write_count(self):\n",
    "         _write_count = self.__dict__.get(\"_write_count_value\", None)\n",
    "         if _write_count is None:\n",
    "-            _write_count = self._write_count_value = mp.Value(\"i\", 0)\n",
    "-        return _write_count.value\n",
    "+            _write_count = self._write_count_value = 0\n",
    "+        return _write_count\n",
    " \n",
    "     @_write_count.setter\n",
    "     def _write_count(self, value):\n",
    "-        _write_count = self.__dict__.get(\"_write_count_value\", None)\n",
    "-        if _write_count is None:\n",
    "-            _write_count = self._write_count_value = mp.Value(\"i\", 0)\n",
    "-        _write_count.value = value\n",
    "+        self._write_count_value = 0\n",
    " \n",
    "     def __getstate__(self):\n",
    "         state = super().__getstate__()\n",
    "@@ -249,7 +243,7 @@ class RoundRobinWriter(Writer):\n",
    "     def __setstate__(self, state):\n",
    "         cursor = state.pop(\"cursor__context\", None)\n",
    "         if cursor is not None:\n",
    "-            _cursor_value = mp.Value(\"i\", cursor)\n",
    "+            _cursor_value = cursor\n",
    "             state[\"_cursor_value\"] = _cursor_value\n",
    "         self.__dict__.update(state)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "I1024 12:22:47.898000 677981 site-packages/torch/_utils_internal.py:116] [27/0_1] CompilationMetrics(compile_id='27/0', frame_key='65', co_name='_rand_given_ndim', co_filename='/home/endopla\n",
    "sm/develop/torchrl-0/torchrl/data/replay_buffers/storages.py', co_firstlineno=164, cache_size=0, accumulated_cache_size=0, guard_count=10, shape_env_guard_count=0, graph_op_count=0, graph_no\n",
    "de_count=0, graph_input_count=0, start_time=1729797767.891117, entire_frame_compile_time_s=0.0068094730377197266, backend_compile_time_s=None, inductor_compile_time_s=None, code_gen_time_s=N\n",
    "one, fail_type=None, fail_reason=None, fail_user_frame_filename=None, fail_user_frame_lineno=None, non_compliant_ops=set(), compliant_custom_ops=set(), restart_reasons={'Graph break due to u\n",
    "nsupported builtin None.SemLock.acquire. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Pyth\n",
    "on builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wra\n",
    "p it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_\n",
    "graph.'}, dynamo_time_before_restart_s=0.0027790069580078125, has_guarded_code=True, possibly_missed_reinplacing_opportunities=0, remote_cache_time_saved_s=0, structured_logging_overhead_s=None, config_suppress_errors=False, config_inline_inbuilt_nn_modules=True, specialize_float=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
