{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding OpenSpiel to TorchRL\n",
    "\n",
    "I need to add OpenSpiel to TorchRL. These are my notes about that.\n",
    "\n",
    "Issue: <https://github.com/pytorch/rl/issues/2133>\n",
    "\n",
    "OpenSpiel: <https://github.com/google-deepmind/open_spiel>\n",
    "\n",
    "OpenSpiel basic API reference:\n",
    "<https://github.com/google-deepmind/open_spiel/blob/master/docs/api_reference.md>\n",
    "\n",
    "This is a very instructive tutorial for how to create a new stateless env:\n",
    "<https://pytorch.org/rl/stable/tutorials/pendulum.html>\n",
    "\n",
    "Action masks: <https://pytorch.org/rl/stable/reference/envs.html#environments-with-masked-actions>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic demo of OpenSpiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspiel\n",
    "chess = pyspiel.load_game('chess')\n",
    "chess_state = chess.new_initial_state()\n",
    "chess_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89,\n",
       " 90,\n",
       " 652,\n",
       " 656,\n",
       " 673,\n",
       " 674,\n",
       " 1257,\n",
       " 1258,\n",
       " 1841,\n",
       " 1842,\n",
       " 2425,\n",
       " 2426,\n",
       " 3009,\n",
       " 3010,\n",
       " 3572,\n",
       " 3576,\n",
       " 3593,\n",
       " 3594,\n",
       " 4177,\n",
       " 4178]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = chess_state.legal_actions()\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a3',\n",
       " 'a4',\n",
       " 'Na3',\n",
       " 'Nc3',\n",
       " 'b3',\n",
       " 'b4',\n",
       " 'c3',\n",
       " 'c4',\n",
       " 'd3',\n",
       " 'd4',\n",
       " 'e3',\n",
       " 'e4',\n",
       " 'f3',\n",
       " 'f4',\n",
       " 'Nf3',\n",
       " 'Nh3',\n",
       " 'g3',\n",
       " 'g4',\n",
       " 'h3',\n",
       " 'h4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chess_state.action_to_string(action) for action in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing move e4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq e3 0 1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_idx = 11\n",
    "print(f'playing move {chess_state.action_to_string(actions[move_idx])}')\n",
    "chess_state.apply_action(actions[move_idx])\n",
    "chess_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspiel.ChessState"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chess_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  89,   90,  652,  656,  673,  674, 1257, 1258, 1841, 1842, 2425,\n",
       "        2426, 3009, 3010, 3572, 3576, 3593, 3594, 4177, 4178]),)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.where(np.array(chess_state.legal_actions_mask()) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHANCE',\n",
       " 'DECISION',\n",
       " 'MEAN_FIELD',\n",
       " 'TERMINAL',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__entries',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__members__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'name',\n",
       " 'value']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chess_state.get_type()\n",
    "\n",
    "dir(pyspiel.StateType.DECISION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes:\n",
    "\n",
    "* The state of a given game type is a derived class of `pyspiel.State`.\n",
    "* Legal actions are encoded as integers, but we can convert them into human-readable format with `pyspiel.<State-derived>.action_to_string()`.\n",
    "* The ``pyspiel.<State-derived>.__repr__()`` method displays the state in some specific format. In the case of `pyspiel.ChessState`, it's a FEN string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing TorchRL env demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import brax.envs\n",
    "#from torchrl.envs import BraxWrapper\n",
    "#base_env = brax.envs.get_environment(\"ant\")\n",
    "#env = BraxWrapper(base_env)\n",
    "#env.set_seed(0)\n",
    "#td = env.reset()\n",
    "#td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "/home/endoplasm/miniconda/envs/torchrl-0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'body': Array([[False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False,  True,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False]], dtype=bool),\n",
       " 'body_state': Array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32),\n",
       " 'head_position': {'row': Array([5, 8], dtype=int32),\n",
       "  'col': Array([5, 8], dtype=int32)},\n",
       " 'tail': Array([[False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False,  True,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False]], dtype=bool),\n",
       " 'fruit_position': {'row': Array([2, 4], dtype=int32),\n",
       "  'col': Array([2, 4], dtype=int32)},\n",
       " 'length': Array(1, dtype=int32),\n",
       " 'step_count': Array(0, dtype=int32),\n",
       " 'action_mask': Array([ True,  True,  True,  True], dtype=bool),\n",
       " 'key': Array([2467461003,  428148500], dtype=uint32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jumanji\n",
    "import jax\n",
    "env = jumanji.make('Snake-v1')\n",
    "key = jax.random.PRNGKey(0)\n",
    "state, _ = env.reset(key)\n",
    "\n",
    "def state_to_dict_of_arrays(state):\n",
    "    res = {}\n",
    "    for key, value in state.items():\n",
    "        if hasattr(value, '_fields'):\n",
    "            res[key] = {}\n",
    "            for field in value._fields:\n",
    "                res[key][field] = jax.numpy.asarray(value)\n",
    "        else:\n",
    "            res[key] = jax.numpy.asarray(value)\n",
    "    \n",
    "    return res\n",
    "\n",
    "state_to_dict_of_arrays(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1r1q1bnr/p1nkppp1/b7/2p4p/1pP4P/3P2P1/PP1KPP2/RNB2BNR w - - 0 11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chess_state = chess.new_initial_state()\n",
    "\n",
    "for _ in range(20):\n",
    "    action = np.random.choice(chess_state.legal_actions())\n",
    "    chess_state.apply_action(action)\n",
    "\n",
    "chess_state.serialize()\n",
    "chess_state.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "I need to add an `OpenSpielWrapper` call to TorchRL, derived from `_EnvWrapper`. I can base it off of the other env wrappers in `torchrl/envs/libs`.\n",
    "\n",
    "The wrapper should be stateless, meaning that an instance of `OpenSpielWrapper` does not hold onto the state (`pyspiel.State`) of the game. Instead, the state of the game should be part of the `TensorDict` that we pass in to `OpenSpielWrapper.step`, and the new state should be part of the output `TensorDict`.\n",
    "\n",
    "So that means that we will need a way to convert a `pyspiel.State` into something that we can put into a `TensorDict`. We will also need a way to reconstruct the `pyspiel.State` from the `TensorDict` so that we can continue to make moves upon each call to `OpenSpielWrapper.step`. What are some ways to do that?\n",
    "\n",
    "I think we may be able to simply place the `pyspiel.State` directly into the `TensorDict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert `pyspiel.State` to `TensorDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        observation: Tensor(shape=torch.Size([1280]), device=cpu, dtype=torch.float64, is_shared=False),\n",
       "        state: NonTensorData(data=1r1q1bnr/p1nkppp1/b7/2p4p/1pP4P/3P2P1/PP1KPP2/RNB2BNR w - - 0 11, batch_size=torch.Size([]), device=None)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensordict import TensorDict\n",
    "\n",
    "def state_to_td(state):\n",
    "  td = TensorDict(\n",
    "    source={\n",
    "      'state': chess_state,\n",
    "      'observation': chess_state.observation_tensor(),\n",
    "    },\n",
    "    batch_size=[],\n",
    "  )\n",
    "  return td\n",
    "\n",
    "td = state_to_td(chess_state)\n",
    "td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenSpiel doesn't support Jax, so we can't do this, like `BraxWrapper` and `JumanjiWrapper` do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchrl.envs.libs.jax_utils import _object_to_tensordict\n",
    "#\n",
    "#state_dict = _object_to_tensordict(chess_state, device='cpu', batch_size=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Plan\n",
    "\n",
    "There is no way to obtain a state dict in OpenSpiel. There is a way to get a string representation of the state of the env and reset the state to the given string representation, so we will use that. However, it would be too inefficient to do this on every single step of `OpenSpielWrapper.step`, so we will have to make it a stateful env. But we can support resetting the `OpenSpielWrapper` to a given string representation of a state in `OpenSpielWrapper.reset`, and that would be good enough to support MCTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 1:\n",
      "FEN: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
      "89\n",
      "1257\n",
      "162\n",
      "1330\n",
      "\n",
      "State 2:\n",
      "FEN: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
      "89\n",
      "1257\n",
      "162\n",
      "1330\n",
      "1841\n",
      "89\n",
      "673\n",
      "16\n",
      "\n",
      "Reloaded state 1:\n",
      "FEN: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
      "89\n",
      "1257\n",
      "162\n",
      "1330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "chess_game = pyspiel.load_game('chess')\n",
    "chess_env = chess_game.new_initial_state()\n",
    "\n",
    "for _ in range(4):\n",
    "    chess_env.apply_action(np.random.choice(chess_env.legal_actions()))\n",
    "\n",
    "state = chess_env.serialize()\n",
    "print('State 1:')\n",
    "print(state)\n",
    "\n",
    "for _ in range(4):\n",
    "    chess_env.apply_action(np.random.choice(chess_env.legal_actions()))\n",
    "\n",
    "print('State 2:')\n",
    "print(chess_env.serialize())\n",
    "\n",
    "chess_env = chess_env.get_game().deserialize_state(state)\n",
    "print('Reloaded state 1:')\n",
    "print(chess_env.serialize())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEN: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
      "89\n",
      "1257\n",
      "162\n",
      "1330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "td = TensorDict(source={'state': state})\n",
    "\n",
    "if 'state' in td:\n",
    "    print(td['state'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chess_env.rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chess_env.is_terminal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplayer games\n",
    "\n",
    "I'be been wondering what the \"observation\" should return for OpenSpiel after a `step` call. Should it only contain the observation for one of the players, or should it contain all of the observations?\n",
    "\n",
    "`PettingZooEnv` has tic-tac-toe, a 2-player game, so I could do whatever it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/endoplasm/develop/torchrl-0/torchrl/envs/libs/pettingzoo.py:1005: UserWarning: PettingZoo failed to load all modules with error message No module named 'multi_agent_ale_py', trying to load individual modules.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td after reset:\n",
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        player_1: TensorDict(\n",
      "            fields={\n",
      "                action_mask: Tensor(shape=torch.Size([1, 9]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                mask: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: TensorDict(\n",
      "                    fields={\n",
      "                        observation: Tensor(shape=torch.Size([1, 3, 3, 2]), device=cpu, dtype=torch.int8, is_shared=False)},\n",
      "                    batch_size=torch.Size([1]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        player_2: TensorDict(\n",
      "            fields={\n",
      "                action_mask: Tensor(shape=torch.Size([1, 9]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                mask: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: TensorDict(\n",
      "                    fields={\n",
      "                        observation: Tensor(shape=torch.Size([1, 3, 3, 2]), device=cpu, dtype=torch.int8, is_shared=False)},\n",
      "                    batch_size=torch.Size([1]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/endoplasm/develop/torchrl-0/torchrl/envs/libs/pettingzoo.py:68: UserWarning: Atari environments failed to load with error message No module named 'multi_agent_ale_py'.\n",
      "  warnings.warn(f\"Atari environments failed to load with error message {err}.\")\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import PettingZooEnv\n",
    "\n",
    "env = PettingZooEnv(\n",
    "    task=\"tictactoe_v3\",\n",
    "    parallel=False,\n",
    "    # A group map allows you to combine the agents into a batched td\n",
    "    #group_map={\"player\": [\"player_1\", \"player_2\"]},\n",
    "    categorical_actions=False,\n",
    "    seed=0,\n",
    "    use_mask=True,\n",
    ")\n",
    "\n",
    "td = env.reset()\n",
    "print('td after reset:')\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand action:\n",
      "TensorDict(\n",
      "    fields={\n",
      "        player_1: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([1, 9]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        player_2: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([1, 9]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=None,\n",
      "            is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "action = env.action_spec.rand()\n",
    "\n",
    "#for _ in range(100):\n",
    "#    action = env.action_spec.rand()\n",
    "#    if (action['player_1', 'action'] == action['player_2', 'action']).all():\n",
    "#        print('yes')\n",
    "print('rand action:')\n",
    "print(action)\n",
    "print(action['player_1', 'action'])\n",
    "print(action['player_2', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td after step:\n",
      "TensorDict(\n",
      "    fields={\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                player_1: TensorDict(\n",
      "                    fields={\n",
      "                        action_mask: Tensor(shape=torch.Size([1, 9]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        mask: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        observation: TensorDict(\n",
      "                            fields={\n",
      "                                observation: Tensor(shape=torch.Size([1, 3, 3, 2]), device=cpu, dtype=torch.int8, is_shared=False)},\n",
      "                            batch_size=torch.Size([1]),\n",
      "                            device=None,\n",
      "                            is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        truncated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([1]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                player_2: TensorDict(\n",
      "                    fields={\n",
      "                        action_mask: Tensor(shape=torch.Size([1, 9]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        mask: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        observation: TensorDict(\n",
      "                            fields={\n",
      "                                observation: Tensor(shape=torch.Size([1, 3, 3, 2]), device=cpu, dtype=torch.int8, is_shared=False)},\n",
      "                            batch_size=torch.Size([1]),\n",
      "                            device=None,\n",
      "                            is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        truncated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([1]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        player_1: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([1, 9]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        player_2: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([1, 9]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=None,\n",
      "            is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "td = env.step(env.action_spec.rand())\n",
    "print('td after step:')\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True, True]])\n",
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True]])\n",
      "tensor([[[[1, 0],\n",
      "          [0, 0],\n",
      "          [0, 0]],\n",
      "\n",
      "         [[0, 0],\n",
      "          [0, 0],\n",
      "          [0, 0]],\n",
      "\n",
      "         [[0, 0],\n",
      "          [0, 0],\n",
      "          [0, 0]]]], dtype=torch.int8)\n",
      "tensor([[False,  True,  True, False,  True,  True,  True,  True,  True]])\n",
      "tensor([[True, True, True, True, True, True, True, True, True]])\n",
      "tensor([[[[1, 0],\n",
      "          [0, 0],\n",
      "          [0, 0]],\n",
      "\n",
      "         [[0, 1],\n",
      "          [0, 0],\n",
      "          [0, 0]],\n",
      "\n",
      "         [[0, 0],\n",
      "          [0, 0],\n",
      "          [0, 0]]]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# What happens if both players do the same action?\n",
    "td = env.reset()\n",
    "\n",
    "action = env.action_spec.rand()\n",
    "\n",
    "#action['player_2', 'action'] = action['player_1', 'action'].clone().detach()\n",
    "\n",
    "td = env.step(action)\n",
    "print(env.action_spec['player_1', 'action'].mask)\n",
    "print(env.action_spec['player_2', 'action'].mask)\n",
    "#print(td['next', 'player_1', 'observation', 'observation'])\n",
    "print(td['next', 'player_1', 'observation', 'observation'])\n",
    "\n",
    "action = env.action_spec.rand()\n",
    "td = env.step(action)\n",
    "print(env.action_spec['player_1', 'action'].mask)\n",
    "print(env.action_spec['player_2', 'action'].mask)\n",
    "#print(td['next', 'player_1', 'observation', 'observation'])\n",
    "print(td['next', 'player_1', 'observation', 'observation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the action spec makes it apparent that there are two players. When we do `env.action_spec.rand()`, a random action is generated for both players, but only the player whose turn it is has their actions masked according to the actual legal actions. Then, when we do `env.step()`, we're only applying the action for the player whose turn it currently is, and the other one gets ignored.\n",
    "\n",
    "`MeltingpotEnv` also seems to support multiplayer games, apparently in the same way, or at least very similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pyspeil.State.apply_moves`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed\n"
     ]
    }
   ],
   "source": [
    "chess_game = pyspiel.load_game('chess')\n",
    "env = chess_game.new_initial_state()\n",
    "action = np.random.choice(env.legal_actions())\n",
    "\n",
    "try:\n",
    "    env.apply_actions_with([action])\n",
    "except AttributeError:\n",
    "    print(\"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, ok, we cannot apply multiple actions at once in OpenSpiel anyway if not all the `State`s implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chess_game = pyspiel.load_game('chess')\n",
    "env = chess_game.new_initial_state()\n",
    "env.current_player()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = np.random.choice(env.legal_actions())\n",
    "env.apply_action(action)\n",
    "env.current_player()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorDict returned by `reset()` versus `step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/endoplasm/miniconda/envs/torchrl-0/lib/python3.9/site-packages/networkx/classes/graph.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping\n",
      "/home/endoplasm/miniconda/envs/torchrl-0/lib/python3.9/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, Set, Iterable\n",
      "/home/endoplasm/miniconda/envs/torchrl-0/lib/python3.9/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, Set, Iterable\n",
      "/home/endoplasm/develop/torchrl-0/torchrl/envs/libs/brax.py:207: UserWarning: No device is set for env BraxWrapper(). Setting a device in Brax wrapped environments is strongly recommended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import brax.envs\n",
    "from torchrl.envs import BraxWrapper\n",
    "base_env = brax.envs.get_environment(\"ant\")\n",
    "env = BraxWrapper(base_env)\n",
    "env.set_seed(0)\n",
    "td = env.reset()\n",
    "td_reset = td.clone()\n",
    "\n",
    "td[\"action\"] = env.action_spec.rand()\n",
    "td = env.step(td)\n",
    "td_step = td.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensordict.utils._StringKeys'>(dict_keys(['observation', 'done', 'terminated', 'state']))\n",
      "---------------\n",
      "<class 'tensordict.utils._StringKeys'>(dict_keys(['observation', 'done', 'terminated', 'state', 'action', 'next']))\n",
      "<class 'tensordict.utils._StringKeys'>(dict_keys(['observation', 'reward', 'done', 'terminated', 'state']))\n"
     ]
    }
   ],
   "source": [
    "print(td_reset.keys())\n",
    "#print(td_reset['state'].keys())\n",
    "print('---------------')\n",
    "print(td_step.keys())\n",
    "#print(td_step['state'].keys())\n",
    "print(td_step['next'].keys())\n",
    "#print(td_step['next', 'state'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/endoplasm/develop/torchrl-0/torchrl/envs/libs/pettingzoo.py:1005: UserWarning: PettingZoo failed to load all modules with error message No module named 'multi_agent_ale_py', trying to load individual modules.\n",
      "  warnings.warn(\n",
      "/home/endoplasm/develop/torchrl-0/torchrl/envs/libs/pettingzoo.py:68: UserWarning: Atari environments failed to load with error message No module named 'multi_agent_ale_py'.\n",
      "  warnings.warn(f\"Atari environments failed to load with error message {err}.\")\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import PettingZooEnv\n",
    "\n",
    "env = PettingZooEnv(\n",
    "    task=\"tictactoe_v3\",\n",
    "    parallel=False,\n",
    "    categorical_actions=False,\n",
    "    seed=0,\n",
    "    use_mask=True,\n",
    ")\n",
    "td = env.reset()\n",
    "td_reset = td.clone()\n",
    "\n",
    "td = env.step(env.action_spec.rand())\n",
    "td_step = td.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: [('player_1', 'action'), ('player_2', 'action')]\n",
      "done_keys: ['done', 'terminated', 'truncated', ('player_1', 'done'), ('player_1', 'terminated'), ('player_1', 'truncated'), ('player_2', 'done'), ('player_2', 'terminated'), ('player_2', 'truncated')]\n",
      "reward_keys: [('player_1', 'reward'), ('player_2', 'reward')]\n",
      "observation_keys: _CompositeSpecKeysView(keys=[('player_1', 'observation', 'observation'), ('player_1', 'action_mask'), ('player_1', 'mask'), ('player_2', 'observation', 'observation'), ('player_2', 'action_mask'), ('player_2', 'mask')])\n",
      "state_keys: _CompositeSpecKeysView(keys=[])\n"
     ]
    }
   ],
   "source": [
    "#print(td_reset.keys())\n",
    "#print('---------------')\n",
    "#print(td_step.keys())\n",
    "#print(td_step['next'].keys())\n",
    "\n",
    "action_keys = env.action_keys\n",
    "done_keys = env.done_keys\n",
    "reward_keys = env.reward_keys\n",
    "observation_keys = env.full_observation_spec.keys(True, True)\n",
    "state_keys = env.full_state_spec.keys(True, True)\n",
    "print(f'action_keys: {action_keys}')\n",
    "print(f'done_keys: {done_keys}')\n",
    "print(f'reward_keys: {reward_keys}')\n",
    "print(f'observation_keys: {observation_keys}')\n",
    "print(f'state_keys: {state_keys}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import OpenSpielEnv\n",
    "\n",
    "env = OpenSpielEnv(\"chess\")\n",
    "td = env.reset()\n",
    "td_reset = td.clone()\n",
    "\n",
    "td = env.step(TensorDict({'action': env.action_spec.rand()}))\n",
    "td_step = td.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: ['action']\n",
      "done_keys: ['done', 'terminated']\n",
      "reward_keys: ['reward']\n",
      "observation_keys: _CompositeSpecKeysView(keys=['observation'])\n",
      "state_keys: ['observation']\n"
     ]
    }
   ],
   "source": [
    "#print(td_reset.keys())\n",
    "#print('---------------')\n",
    "#print(td_step.keys())\n",
    "#print(td_step['next'].keys())\n",
    "\n",
    "action_keys = env.action_keys\n",
    "done_keys = env.done_keys\n",
    "reward_keys = env.reward_keys\n",
    "observation_keys = env.full_observation_spec.keys(True, True)\n",
    "state_keys = list(env.full_state_spec.keys(True, True))\n",
    "print(f'action_keys: {action_keys}')\n",
    "print(f'done_keys: {done_keys}')\n",
    "print(f'reward_keys: {reward_keys}')\n",
    "print(f'observation_keys: {observation_keys}')\n",
    "print(f'state_keys: {state_keys}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the TensorDict returned by `step` needs to have 'observation' in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The sets of keys in the tensordicts to stack are exclusive. Consider using `LazyStackedTensorDict.maybe_dense_stack` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/develop/tensordict-0/tensordict/_torch_func.py:444\u001b[0m, in \u001b[0;36m_stack\u001b[0;34m(list_of_tensordicts, dim, device, out, strict, contiguous, maybe_dense_stack)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[43m_check_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_of_tensordicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/develop/tensordict-0/tensordict/utils.py:1712\u001b[0m, in \u001b[0;36m_check_keys\u001b[0;34m(list_of_tensordicts, strict, include_nested, leaves_only)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m keys:\n\u001b[0;32m-> 1712\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1713\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(td\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which are incompatible\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1714\u001b[0m             )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys\n",
      "\u001b[0;31mKeyError\u001b[0m: \"got keys {'observation', 'action', 'reward', 'current_player', 'next', 'done', 'terminated', 'state'} and {'observation', 'action', 'current_player', 'next', 'done', 'terminated', 'state'} which are incompatible\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/develop/torchrl-0/torchrl/envs/common.py:2594\u001b[0m, in \u001b[0;36mEnvBase.rollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, return_contiguous, tensordict, set_truncated, out)\u001b[0m\n\u001b[1;32m   2592\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollout_nonstop(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2593\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;28;01mif\u001b[39;00m tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m-> 2594\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensordicts)\n\u001b[1;32m   2595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_contiguous:\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/develop/tensordict-0/tensordict/base.py:405\u001b[0m, in \u001b[0;36mTensorDictBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m TD_HANDLED_FUNCTIONS \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28missubclass\u001b[39m(t, (Tensor, TensorDictBase)) \u001b[38;5;129;01mor\u001b[39;00m _is_tensorclass(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types\n\u001b[1;32m    403\u001b[0m ):\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m--> 405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTD_HANDLED_FUNCTIONS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/develop/tensordict-0/tensordict/_torch_func.py:451\u001b[0m, in \u001b[0;36m_stack\u001b[0;34m(list_of_tensordicts, dim, device, out, strict, contiguous, maybe_dense_stack)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _stack(list_of_tensordicts, dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    452\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sets of keys in the tensordicts to stack are exclusive. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using `LazyStackedTensorDict.maybe_dense_stack` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(_tensordict, LazyStackedTensorDict)\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _tensordict \u001b[38;5;129;01min\u001b[39;00m list_of_tensordicts\n\u001b[1;32m    460\u001b[0m ):\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Let's try to see if all tensors have the same shape\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# If so, we can assume that we can densly stack the sub-tds\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The sets of keys in the tensordicts to stack are exclusive. Consider using `LazyStackedTensorDict.maybe_dense_stack` instead."
     ]
    }
   ],
   "source": [
    "env.rollout(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'observation', 'action', 'reward', 'current_player', 'next', 'done', 'terminated', 'state'}\n",
    "{'observation', 'action',           'current_player', 'next', 'done', 'terminated', 'state'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSpiel games info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blotto: Dynamics.SIMULTANEOUS\n",
      "coop_box_pushing: Dynamics.SIMULTANEOUS\n",
      "goofspiel: Dynamics.SIMULTANEOUS\n",
      "laser_tag: Dynamics.SIMULTANEOUS\n",
      "markov_soccer: Dynamics.SIMULTANEOUS\n",
      "matching_pennies_3p: Dynamics.SIMULTANEOUS\n",
      "matrix_bos: Dynamics.SIMULTANEOUS\n",
      "matrix_brps: Dynamics.SIMULTANEOUS\n",
      "matrix_cd: Dynamics.SIMULTANEOUS\n",
      "matrix_coordination: Dynamics.SIMULTANEOUS\n",
      "matrix_mp: Dynamics.SIMULTANEOUS\n",
      "matrix_pd: Dynamics.SIMULTANEOUS\n",
      "matrix_rps: Dynamics.SIMULTANEOUS\n",
      "matrix_rpsw: Dynamics.SIMULTANEOUS\n",
      "matrix_sh: Dynamics.SIMULTANEOUS\n",
      "matrix_shapleys_game: Dynamics.SIMULTANEOUS\n",
      "mfg_crowd_modelling: Dynamics.MEAN_FIELD\n",
      "mfg_crowd_modelling_2d: Dynamics.MEAN_FIELD\n",
      "mfg_dynamic_routing: Dynamics.MEAN_FIELD\n",
      "mfg_garnet: Dynamics.MEAN_FIELD\n",
      "nfg_game: Dynamics.SIMULTANEOUS\n",
      "normal_form_extensive_game: Dynamics.SIMULTANEOUS\n",
      "oshi_zumo: Dynamics.SIMULTANEOUS\n",
      "pathfinding: Dynamics.SIMULTANEOUS\n",
      "repeated_game: Dynamics.SIMULTANEOUS\n"
     ]
    }
   ],
   "source": [
    "import pyspiel\n",
    "\n",
    "def game_info_str(game_type):\n",
    "    dynamics = game_type.dynamics\n",
    "    name = game_type.short_name\n",
    "\n",
    "    return (\n",
    "        f\"{name}: {dynamics}\"\n",
    "    )\n",
    "\n",
    "\n",
    "for game_type in pyspiel.registered_games():\n",
    "    if game_type.dynamics != pyspiel.GameType.Dynamics.SEQUENTIAL:\n",
    "        print(game_info_str(game_type))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO list\n",
    "\n",
    "* Support chance nodes: <https://openspiel.readthedocs.io/en/latest/concepts.html#playing-a-trajectory>\n",
    "\n",
    "* Change specs to be like PettingZooEnv, as shown in the exploration above. Specifically, support all agents acting at each step, but for games where players act sequentially, use a mask to mask out all but the player whose turn it currently is. PettingZooEnv docs explain it: <https://pytorch.org/rl/stable/reference/generated/torchrl.envs.PettingZooEnv.html>\n",
    "\n",
    "* (Maybe) make some more documentation since OpenSpiel has a fair amount of important concepts that people would need to be aware of to use it properly. Could base docs off of existing OpenSpiel docs: <https://openspiel.readthedocs.io/en/latest/concepts.html>. Alternatively, just link to the relevant OpenSpiel docs.\n",
    "\n",
    "* \"cursor_go\" and \"cliff_walking\" fail tests with a similar error. Hunch is that the game reaches the end state and I'm not detecting it quite correctly.\n",
    "\n",
    "* Explanation of when `current_player() < 0` and why: <https://openspiel.readthedocs.io/en/latest/api_reference/state_current_player.html?highlight=players#openspiel-state-methods-current-player>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
