{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO notes\n",
    "\n",
    "These are my notes on the TorchRL PPO tutorial: <https://pytorch.org/rl/stable/tutorials/coding_ppo.html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchrl\n",
    "import torch\n",
    "import tensordict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device('cuda:0')\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device('cpu')\n",
    ")\n",
    "\n",
    "# Number of cells in each layer\n",
    "num_cells = 256\n",
    "\n",
    "# Learning rate\n",
    "lr = 3e-4\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "\n",
    "# Number of frames in the whole training session.\n",
    "# Frames are just steps in the simulation.\n",
    "# May need to increase this significantly for real training.\n",
    "total_frames = 20_000\n",
    "\n",
    "# Number of frames in a batch collection.\n",
    "# We'll collect this many frames and then perform the training\n",
    "# optimization on them.\n",
    "frames_per_batch = 1000\n",
    "\n",
    "# Number of frames in a sub-batch.\n",
    "# We split each batch into smaller sub-batches in the training loop.\n",
    "# (But why though?)\n",
    "sub_batch_size = 64\n",
    "\n",
    "# Number of training epochs per batch.\n",
    "# After collecting a particular batch, we run the optimization on it\n",
    "# multiple times in a row. Each time is called an epoch.\n",
    "num_epochs = 10\n",
    "\n",
    "# Clip value for PPO loss\n",
    "clip_epsilon = 0.2\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "lmbda = 0.95\n",
    "\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = torchrl.envs.GymEnv(\n",
    "    \"InvertedPendulum-v4\",\n",
    "    device=device,\n",
    "    render_mode=\"human\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = torchrl.envs.TransformedEnv(\n",
    "    base_env,\n",
    "    torchrl.envs.Compose(\n",
    "        # Normalize the observations to loosely match a Gaussian dist.\n",
    "        torchrl.envs.ObservationNorm(in_keys=[\"observation\"]),\n",
    "\n",
    "        # Convert to float for better performance.\n",
    "        torchrl.envs.DoubleToFloat(),\n",
    "\n",
    "        # This will allow us to count the frames.\n",
    "        torchrl.envs.StepCounter(),\n",
    "    )\n",
    ")\n",
    "\n",
    "# This sets up the normalization to run 1000 random steps, and those\n",
    "# steps will be used to automatically set internal parameters so that\n",
    "# the observations fit a Gaussian curve.\n",
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization constant shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: Composite(\n",
      "    observation: UnboundedContinuous(\n",
      "        shape=torch.Size([4]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([4]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([4]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    step_count: BoundedDiscrete(\n",
      "        shape=torch.Size([1]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True)),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.int64,\n",
      "        domain=discrete),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuous(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "    device=cuda:0,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n",
      "input_spec: Composite(\n",
      "    full_state_spec: Composite(\n",
      "        step_count: BoundedDiscrete(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([])),\n",
      "    full_action_spec: Composite(\n",
      "        action: BoundedContinuous(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([])),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([]))\n",
      "action_spec (as defined by input_spec): BoundedContinuous(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "    device=cuda:0,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 18:22:55,227 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "torchrl.envs.check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([3, 4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                step_count: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3, 4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        step_count: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n",
      "Shape of the rollout TensorDict: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(3)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do PPO, we need to set up a stochastic policy for exploration. We'll create an actor module for this. The output of the network used by the actor needs to be a distribution for the action to take, rather than a single action value.\n",
    "\n",
    "The distribution is centered on a value, \"loc\", with a variation, \"scale\", which represents how wide the distribution is. When making an action, we'll pick a random sample according to this distribution.\n",
    "\n",
    "The action spec says that the action value is continuous. It is common to use a Tanh normal distribution for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the policy network\n",
    "actor_net = torch.nn.Sequential(\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    tensordict.nn.distributions.NormalParamExtractor(),\n",
    ")\n",
    "\n",
    "# Need to wrap the network with a TensorDictModule to be compatible\n",
    "# with the actor.\n",
    "policy_module = tensordict.nn.TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")\n",
    "\n",
    "# Wrap the module in a probabilistic actor, which knows how to\n",
    "# pick an action according to the given distribution.\n",
    "policy_module = torchrl.modules.ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=torchrl.modules.TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": env.action_spec.space.low,\n",
    "        \"high\": env.action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # Apparently, we'll need the log-prob for the numerator of the importance\n",
    "    # weights, but I don't know what importance weights are.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a value module which estimates the return of a trajectory. After this this network is trained, it will be able to estimate the values of actions before actually taking the action. We'll give this network almost the same structure as the actor policy, but it won't have the separate distribution values--there will just be one output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = torch.nn.Sequential(\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = torchrl.modules.ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the policy and value modules once to make sure they are set up properly and that they output the correct fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        loc: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        scale: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n",
      "Running value: TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        state_value: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Running policy:\", policy_module(env.reset()))\n",
    "print(\"Running value:\", value_module(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to set up a data collector, which has the following responsibilities: reset the environment, compute an action based on the last observation, execute a step in the env, repeat until the environment is done.\n",
    "\n",
    "`SyncDataCollector` is the simplest one. The collector will return a batch of frames that we can train with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = torchrl.collectors.SyncDataCollector(\n",
    "    # The env to act in and collect from\n",
    "    env,\n",
    "\n",
    "    # The policy to use for making decisions\n",
    "    policy_module,\n",
    "\n",
    "    # The number of frames to collect at each iteration\n",
    "    frames_per_batch=frames_per_batch,\n",
    "\n",
    "    # The maximum number of frames to go before resetting the env\n",
    "    total_frames=total_frames,\n",
    "\n",
    "    # With this setting, if there's a reset, the trajectories are just\n",
    "    # concatenated together rather than split into separate arrays.\n",
    "    split_trajs=False,\n",
    "\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set up a replay buffer to store the collected data and allow us to access it in the future for training. We'll just store a single batch of data in our replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = torchrl.data.ReplayBuffer(\n",
    "    # This replay buffer will only hold one batch of data\n",
    "    storage=torchrl.data.LazyTensorStorage(\n",
    "        max_size=frames_per_batch,\n",
    "    ),\n",
    "\n",
    "    # With this sampler, with each sample we take, the frames will\n",
    "    # be shuffled, and each frame can only be sampled once.\n",
    "    sampler=torchrl.data.SamplerWithoutReplacement(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a loss function to determine how well our agent is doing. We'll use a standard loss module for PPO called `ClipPPOLoss`. To use this module, we need to pass batches from our value module into something called an advantage module, and then pass the result onto `ClipPPOLoss`. We'll use GAE for the advantage module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage_module = torchrl.objectives.value.GAE(\n",
    "    gamma=gamma,\n",
    "    lmbda=lmbda,\n",
    "    value_network=value_module,\n",
    "    average_gae=True,\n",
    ")\n",
    "\n",
    "loss_module = torchrl.objectives.ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "loss_module.set_keys(\n",
    "    reward='reward'\n",
    ")\n",
    "\n",
    "# Create an optimizer for the loss module\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim,\n",
    "    total_frames // frames_per_batch,\n",
    "    0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to write the training loop. The steps are:\n",
    "\n",
    "* Collect data\n",
    "  * Compute advantage\n",
    "    * Loop over the collected data to compute loss\n",
    "    * Back propagate\n",
    "    * Optimize\n",
    "    * Repeat\n",
    "  * Repeat\n",
    "* Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000/20000 0:00:10s] eval: (reward sum: 43.0, reward mean: 1.0, step count: 42), collection: (max step count: 19)\n",
      "[2000/20000 0:00:19s] eval: (reward sum: 63.0, reward mean: 1.0, step count: 62), collection: (max step count: 40)\n",
      "[3000/20000 0:00:28s] eval: (reward sum: 85.0, reward mean: 1.0, step count: 84), collection: (max step count: 61)\n",
      "[4000/20000 0:00:37s] eval: (reward sum: 83.0, reward mean: 0.9999999403953552, step count: 82), collection: (max step count: 83)\n",
      "[5000/20000 0:00:47s] eval: (reward sum: 289.0, reward mean: 1.0, step count: 288), collection: (max step count: 94)\n",
      "[6000/20000 0:01:03s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 147)\n",
      "[7000/20000 0:01:18s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 128)\n",
      "[8000/20000 0:01:33s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 316)\n",
      "[9000/20000 0:01:47s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 257)\n",
      "[10000/20000 0:02:02s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 403)\n",
      "[11000/20000 0:02:17s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 202)\n",
      "[12000/20000 0:02:32s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 421)\n",
      "[13000/20000 0:02:47s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 998)\n",
      "[14000/20000 0:03:02s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 999)\n",
      "[15000/20000 0:03:17s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 998)\n",
      "[16000/20000 0:03:31s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 999)\n",
      "[17000/20000 0:03:46s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 999)\n",
      "[18000/20000 0:04:01s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 999)\n",
      "[19000/20000 0:04:16s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 999)\n",
      "[20000/20000 0:04:26s] eval: (reward sum: 1000.0, reward mean: 1.0, step count: 999), collection: (max step count: 999)\n"
     ]
    }
   ],
   "source": [
    "cur_frames = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Collect batches of data\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # Train on the collected batch\n",
    "    for _ in range(num_epochs):\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "\n",
    "        # During each epoch, a batch of data is trained in sub-batches\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Back propogate and optimize\n",
    "            loss_value.backward()\n",
    "\n",
    "            # Bounding the grad is good practice\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(),\n",
    "                max_grad_norm\n",
    "            )\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    # That's all the training we need!\n",
    "    # But let's monitor how well our policy is working.\n",
    "\n",
    "    reward_sum = tensordict_data['next', 'reward'].sum().item()\n",
    "    max_step_count = tensordict_data[\"step_count\"].max().item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Execute a rollout with the trained policy\n",
    "        eval_rollout = env.rollout(10_000, policy_module)\n",
    "        eval_reward = eval_rollout[\"next\", \"reward\"]\n",
    "        eval_reward_sum = eval_reward.sum().item()\n",
    "        eval_reward_mean = eval_reward.mean().item()\n",
    "        eval_step_count = eval_rollout[\"step_count\"].max().item()\n",
    "\n",
    "        cur_frames += tensordict_data.numel()\n",
    "        run_time = str(datetime.now() - start_time).split('.')[0]\n",
    "\n",
    "        eval_str = (\n",
    "            f\"[{cur_frames}/{total_frames}\"\n",
    "            f\" {run_time}s]\"\n",
    "            f\" eval: (reward sum: {eval_reward_sum}\"\n",
    "            f\", reward mean: {eval_reward_mean}\"\n",
    "            f\", step count: {eval_step_count}\"\n",
    "            f\"), collection: (max step count: {max_step_count})\"\n",
    "        )\n",
    "        print(eval_str)\n",
    "\n",
    "    # This controls the learning rate scheduler, which is good practice\n",
    "    # to include\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that according to the [docs](https://www.gymlibrary.dev/environments/mujoco/inverted_pendulum/) for \"InvertedPendulum-v4\", the environnment automatically resets at step 1000, so 1000 is the maximum possible reward sum. This model is fairly well trained after just a few minutes, since we're consistently reaching the maximum reward when we evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl-mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
