{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO notes\n",
    "\n",
    "These are my notes on the TorchRL PPO tutorial: <https://pytorch.org/rl/stable/tutorials/coding_ppo.html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchrl\n",
    "import torch\n",
    "import tensordict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device('cuda:0')\n",
    "    #if torch.cuda.is_available()\n",
    "    if False\n",
    "    else torch.device('cpu')\n",
    ")\n",
    "\n",
    "# Number of cells in each layer\n",
    "num_cells = 256\n",
    "\n",
    "# Learning rate\n",
    "lr = 3e-4\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "\n",
    "# Number of frames in the whole training session.\n",
    "# Frames are just steps in the simulation.\n",
    "# May need to increase this significantly for real training.\n",
    "total_frames = 20_000\n",
    "\n",
    "# Number of frames in a batch collection.\n",
    "# We'll collect this many frames and then perform the training\n",
    "# optimization on them.\n",
    "frames_per_batch = 100\n",
    "\n",
    "# Number of frames in a sub-batch.\n",
    "# We split each batch into smaller sub-batches in the training loop.\n",
    "# (But why though?)\n",
    "sub_batch_size = 64\n",
    "\n",
    "# Number of training epochs per batch.\n",
    "# After collecting a particular batch, we run the optimization on it\n",
    "# multiple times in a row. Each time is called an epoch.\n",
    "num_epochs = 10\n",
    "\n",
    "# Clip value for PPO loss\n",
    "clip_epsilon = 0.2\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "lmbda = 0.95\n",
    "\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/endoplasm/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/mlagents_envs/environment.py:94: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  unity_communicator_version = StrictVersion(unity_com_ver)\n"
     ]
    }
   ],
   "source": [
    "#base_env = torchrl.envs.GymEnv(\n",
    "#    \"InvertedPendulum-v4\",\n",
    "#    device=device,\n",
    "#    render_mode=\"human\"\n",
    "#)\n",
    "base_env = torchrl.envs.UnityMLAgentsEnv(\n",
    "    #registered_name='3DBall'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        group_0: TensorDict(\n",
      "            fields={\n",
      "                agent_0: TensorDict(\n",
      "                    fields={\n",
      "                        VectorSensor_size8: Tensor(shape=torch.Size([8]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([]),\n",
      "                    device=None,\n",
      "                    is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = base_env.reset()\n",
    "print(td)\n",
    "\n",
    "#td = base_env.step(base_env.action_spec.rand())\n",
    "#print(td['next'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Composite(\n",
       "    group_0: Composite(\n",
       "        agent_0: Composite(\n",
       "            VectorSensor_size8: UnboundedContinuous(\n",
       "                shape=torch.Size([8]),\n",
       "                space=ContinuousBox(\n",
       "                    low=Tensor(shape=torch.Size([8]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "                    high=Tensor(shape=torch.Size([8]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "                device=cpu,\n",
       "                dtype=torch.float32,\n",
       "                domain=continuous),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        device=None,\n",
       "        shape=torch.Size([])),\n",
       "    device=None,\n",
       "    shape=torch.Size([]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_env.observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/endoplasm/develop/torchrl-mlagents/torchrl/envs/libs/unity_mlagents.py:381: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "env = torchrl.envs.TransformedEnv(\n",
    "    base_env,\n",
    "    torchrl.envs.Compose(\n",
    "        # Normalize the observations to loosely match a Gaussian dist.\n",
    "        torchrl.envs.ObservationNorm(in_keys=[(\"group_0\", \"agent_0\", \"VectorSensor_size8\")]),\n",
    "\n",
    "        # Convert to float for better performance.\n",
    "        torchrl.envs.DoubleToFloat(),\n",
    "\n",
    "        # This will allow us to count the frames.\n",
    "        #torchrl.envs.StepCounter(),\n",
    "    )\n",
    ")\n",
    "\n",
    "# This sets up the normalization to run 100 random steps, and those\n",
    "# steps will be used to automatically set internal parameters so that\n",
    "# the observations fit a Gaussian curve.\n",
    "env.transform[0].init_stats(num_iter=100, reduce_dim=0, cat_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization constant shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: Composite(\n",
      "    group_0: Composite(\n",
      "        agent_0: Composite(\n",
      "            VectorSensor_size8: UnboundedContinuous(\n",
      "                shape=torch.Size([8]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([8]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([8]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            device=None,\n",
      "            shape=torch.Size([])),\n",
      "        device=None,\n",
      "        shape=torch.Size([])),\n",
      "    device=None,\n",
      "    shape=torch.Size([]))\n",
      "reward_spec: Composite(\n",
      "    group_0: Composite(\n",
      "        agent_0: Composite(\n",
      "            reward: UnboundedContinuous(\n",
      "                shape=torch.Size([1]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            group_reward: UnboundedContinuous(\n",
      "                shape=torch.Size([1]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            device=None,\n",
      "            shape=torch.Size([])),\n",
      "        device=None,\n",
      "        shape=torch.Size([])),\n",
      "    device=None,\n",
      "    shape=torch.Size([]))\n",
      "input_spec: Composite(\n",
      "    full_state_spec: Composite(\n",
      "    ,\n",
      "        device=None,\n",
      "        shape=torch.Size([])),\n",
      "    full_action_spec: Composite(\n",
      "        group_0: Composite(\n",
      "            agent_0: Composite(\n",
      "                continuous_action: BoundedContinuous(\n",
      "                    shape=torch.Size([2]),\n",
      "                    space=ContinuousBox(\n",
      "                        low=Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                        high=Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "                    device=cpu,\n",
      "                    dtype=torch.float32,\n",
      "                    domain=continuous),\n",
      "                device=None,\n",
      "                shape=torch.Size([])),\n",
      "            device=None,\n",
      "            shape=torch.Size([])),\n",
      "        device=None,\n",
      "        shape=torch.Size([])),\n",
      "    device=None,\n",
      "    shape=torch.Size([]))\n",
      "action_spec (as defined by input_spec): BoundedContinuous(\n",
      "    shape=torch.Size([2]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "    device=cpu,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 12:41:45,766 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "torchrl.envs.check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        group_0: TensorDict(\n",
      "            fields={\n",
      "                agent_0: TensorDict(\n",
      "                    fields={\n",
      "                        VectorSensor_size8: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        continuous_action: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([3]),\n",
      "                    device=None,\n",
      "                    is_shared=False)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                group_0: TensorDict(\n",
      "                    fields={\n",
      "                        agent_0: TensorDict(\n",
      "                            fields={\n",
      "                                VectorSensor_size8: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                                group_reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                            batch_size=torch.Size([3]),\n",
      "                            device=None,\n",
      "                            is_shared=False)},\n",
      "                    batch_size=torch.Size([3]),\n",
      "                    device=None,\n",
      "                    is_shared=False)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=None,\n",
      "            is_shared=False)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(3)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do PPO, we need to set up a stochastic policy for exploration. We'll create an actor module for this. The output of the network used by the actor needs to be a distribution for the action to take, rather than a single action value.\n",
    "\n",
    "The distribution is centered on a value, \"loc\", with a variation, \"scale\", which represents how wide the distribution is. When making an action, we'll pick a random sample according to this distribution.\n",
    "\n",
    "The action spec says that the action value is continuous. It is common to use a Tanh normal distribution for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action_spec = env.action_spec[(\"group_0\", \"agent_0\", \"continuous_action\"]\n",
    "action_spec = env.action_spec\n",
    "\n",
    "# Create the policy network\n",
    "actor_net = torch.nn.Sequential(\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(2 * action_spec.shape[-1], device=device),\n",
    "    tensordict.nn.distributions.NormalParamExtractor(),\n",
    ")\n",
    "\n",
    "# Need to wrap the network with a TensorDictModule to be compatible\n",
    "# with the actor.\n",
    "policy_module = tensordict.nn.TensorDictModule(\n",
    "    actor_net, in_keys=[(\"group_0\", \"agent_0\", \"VectorSensor_size8\")], out_keys=[\"loc\", \"scale\"]\n",
    ")\n",
    "\n",
    "# Wrap the module in a probabilistic actor, which knows how to\n",
    "# pick an action according to the given distribution.\n",
    "policy_module = torchrl.modules.ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    #spec=env.action_spec,\n",
    "    spec=action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    out_keys=[(\"group_0\", \"agent_0\", \"continuous_action\")],\n",
    "    distribution_class=torchrl.modules.TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": action_spec.space.low,\n",
    "        \"high\": action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # Apparently, we'll need the log-prob for the numerator of the importance\n",
    "    # weights, but I don't know what importance weights are.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a value module which estimates the return of a trajectory. After this this network is trained, it will be able to estimate the values of actions before actually taking the action. We'll give this network almost the same structure as the actor policy, but it won't have the separate distribution values--there will just be one output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = torch.nn.Sequential(\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(num_cells, device=device),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = torchrl.modules.ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[(\"group_0\", \"agent_0\", \"VectorSensor_size8\")],\n",
    "    out_keys=[(\"group_0\", \"agent_0\", \"continuous_action\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the policy and value modules once to make sure they are set up properly and that they output the correct fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: TensorDict(\n",
      "    fields={\n",
      "        group_0: TensorDict(\n",
      "            fields={\n",
      "                agent_0: TensorDict(\n",
      "                    fields={\n",
      "                        VectorSensor_size8: Tensor(shape=torch.Size([8]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        continuous_action: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([]),\n",
      "                    device=None,\n",
      "                    is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        loc: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        scale: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "Running value: TensorDict(\n",
      "    fields={\n",
      "        group_0: TensorDict(\n",
      "            fields={\n",
      "                agent_0: TensorDict(\n",
      "                    fields={\n",
      "                        VectorSensor_size8: Tensor(shape=torch.Size([8]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        continuous_action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([]),\n",
      "                    device=None,\n",
      "                    is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        loc: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        scale: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.reset()\n",
    "print(\"Running policy:\", policy_module(td))\n",
    "print(\"Running value:\", value_module(td))\n",
    "\n",
    "td = env.step(env.full_action_spec.rand())['next']\n",
    "#print(\"Running policy:\", policy_module(td))\n",
    "#print(\"Running value:\", value_module(td))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to set up a data collector, which has the following responsibilities: reset the environment, compute an action based on the last observation, execute a step in the env, repeat until the environment is done.\n",
    "\n",
    "`SyncDataCollector` is the simplest one. The collector will return a batch of frames that we can train with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = torchrl.collectors.SyncDataCollector(\n",
    "    # The env to act in and collect from\n",
    "    env,\n",
    "\n",
    "    # The policy to use for making decisions\n",
    "    policy_module,\n",
    "    #out_keys=[('group_0', 'agent_0', 'continuous_action')],\n",
    "\n",
    "    # The number of frames to collect at each iteration\n",
    "    frames_per_batch=frames_per_batch,\n",
    "\n",
    "    # The maximum number of frames to go before resetting the env\n",
    "    total_frames=total_frames,\n",
    "\n",
    "    # With this setting, if there's a reset, the trajectories are just\n",
    "    # concatenated together rather than split into separate arrays.\n",
    "    split_trajs=False,\n",
    "\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([100]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        group_0: TensorDict(\n",
       "            fields={\n",
       "                agent_0: TensorDict(\n",
       "                    fields={\n",
       "                        VectorSensor_size8: Tensor(shape=torch.Size([100, 8]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        continuous_action: Tensor(shape=torch.Size([100, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        done: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                        terminated: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                        truncated: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "                    batch_size=torch.Size([100]),\n",
       "                    device=cpu,\n",
       "                    is_shared=False)},\n",
       "            batch_size=torch.Size([100]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        loc: Tensor(shape=torch.Size([100, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                group_0: TensorDict(\n",
       "                    fields={\n",
       "                        agent_0: TensorDict(\n",
       "                            fields={\n",
       "                                VectorSensor_size8: Tensor(shape=torch.Size([100, 8]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                done: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                                group_reward: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                reward: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                terminated: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                                truncated: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "                            batch_size=torch.Size([100]),\n",
       "                            device=cpu,\n",
       "                            is_shared=False)},\n",
       "                    batch_size=torch.Size([100]),\n",
       "                    device=cpu,\n",
       "                    is_shared=False)},\n",
       "            batch_size=torch.Size([100]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        sample_log_prob: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        scale: Tensor(shape=torch.Size([100, 2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "    batch_size=torch.Size([100]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector.rollout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set up a replay buffer to store the collected data and allow us to access it in the future for training. We'll just store a single batch of data in our replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = torchrl.data.ReplayBuffer(\n",
    "    # This replay buffer will only hold one batch of data\n",
    "    storage=torchrl.data.LazyTensorStorage(\n",
    "        max_size=frames_per_batch,\n",
    "    ),\n",
    "\n",
    "    # With this sampler, with each sample we take, the frames will\n",
    "    # be shuffled, and each frame can only be sampled once.\n",
    "    sampler=torchrl.data.SamplerWithoutReplacement(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a loss function to determine how well our agent is doing. We'll use a standard loss module for PPO called `ClipPPOLoss`. To use this module, we need to pass batches from our value module into something called an advantage module, and then pass the result onto `ClipPPOLoss`. We'll use GAE for the advantage module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#advantage_module = torchrl.objectives.value.GAE(\n",
    "#    gamma=gamma,\n",
    "#    lmbda=lmbda,\n",
    "#    value_network=value_module,\n",
    "#    #value_network=value_thing,\n",
    "#    average_gae=True,\n",
    "#)\n",
    "\n",
    "loss_module = torchrl.objectives.ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "loss_module.set_keys(\n",
    "    reward=('group_0', 'agent_0', 'reward'),\n",
    "    #value=('group_0', 'agent_0', 'VectorSensor_size8')\n",
    "    value=('group_0', 'agent_0', 'continuous_action')\n",
    ")\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    torchrl.objectives.ValueEstimators.GAE,\n",
    "    gamma=gamma,\n",
    "    lmbda=lmbda,\n",
    ")\n",
    "\n",
    "advantage_module = loss_module.value_estimator\n",
    "\n",
    "# Create an optimizer for the loss module\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim,\n",
    "    total_frames // frames_per_batch,\n",
    "    0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = env.reset()\n",
    "td = env.step(env.full_action_spec.rand())\n",
    "#print(advantage_module(td))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to write the training loop. The steps are:\n",
    "\n",
    "* Collect data\n",
    "  * Compute advantage\n",
    "    * Loop over the collected data to compute loss\n",
    "    * Back propagate\n",
    "    * Optimize\n",
    "    * Repeat\n",
    "  * Repeat\n",
    "* Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The sets of keys in the tensordicts to stack are exclusive. Consider using `LazyStackedTensorDict.maybe_dense_stack` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/_torch_func.py:455\u001b[0m, in \u001b[0;36m_stack\u001b[0;34m(list_of_tensordicts, dim, device, out, strict, contiguous, maybe_dense_stack)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 455\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[43m_check_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_of_tensordicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/utils.py:1671\u001b[0m, in \u001b[0;36m_check_keys\u001b[0;34m(list_of_tensordicts, strict, include_nested, leaves_only)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m keys:\n\u001b[0;32m-> 1671\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1672\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(td\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which are incompatible\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1673\u001b[0m             )\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys\n",
      "\u001b[0;31mKeyError\u001b[0m: \"got keys {'VectorSensor_size8', 'continuous_action'} and {'VectorSensor_size8'} which are incompatible\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tensordict_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(collector):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Train on the collected batch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 10\u001b[0m         \u001b[43madvantage_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_network_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_critic_network_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         data_view \u001b[38;5;241m=\u001b[39m tensordict_data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m         replay_buffer\u001b[38;5;241m.\u001b[39mextend(data_view\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/develop/torchrl-mlagents/torchrl/objectives/value/advantages.py:67\u001b[0m, in \u001b[0;36m_self_set_skip_existing.<locals>.new_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_skip_existing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_existing):\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/develop/torchrl-mlagents/torchrl/objectives/value/advantages.py:56\u001b[0m, in \u001b[0;36m_self_set_grad_enabled.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fun)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdifferentiable):\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/nn/common.py:325\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/develop/torchrl-mlagents/torchrl/objectives/value/advantages.py:1333\u001b[0m, in \u001b[0;36mGAE.forward\u001b[0;34m(self, tensordict, params, target_params, time_dim)\u001b[0m\n\u001b[1;32m   1327\u001b[0m             target_params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mclone(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hold_out_net(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_network) \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1329\u001b[0m         params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m target_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     ) \u001b[38;5;28;01melse\u001b[39;00m nullcontext():\n\u001b[1;32m   1331\u001b[0m         \u001b[38;5;66;03m# we may still need to pass gradient, but we don't want to assign grads to\u001b[39;00m\n\u001b[1;32m   1332\u001b[0m         \u001b[38;5;66;03m# value net params\u001b[39;00m\n\u001b[0;32m-> 1333\u001b[0m         value, next_value \u001b[38;5;241m=\u001b[39m \u001b[43m_call_value_nets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_network\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnext_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m            \u001b[49m\u001b[43msingle_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshifted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_keys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdetach_next\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvmap_randomness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap_randomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     value \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_keys\u001b[38;5;241m.\u001b[39mvalue)\n",
      "File \u001b[0;32m~/develop/torchrl-mlagents/torchrl/objectives/value/advantages.py:131\u001b[0m, in \u001b[0;36m_call_value_nets\u001b[0;34m(value_net, data, params, next_params, single_call, value_key, detach_next, vmap_randomness)\u001b[0m\n\u001b[1;32m    129\u001b[0m     value, value_ \u001b[38;5;241m=\u001b[39m value_est[idx], value_est[idx_]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     data_in \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m^\u001b[39m (next_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams and next_params must be either both provided or not.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/base.py:470\u001b[0m, in \u001b[0;36mTensorDictBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m TD_HANDLED_FUNCTIONS \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28missubclass\u001b[39m(t, (Tensor, TensorDictBase)) \u001b[38;5;129;01mor\u001b[39;00m _is_tensorclass(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types\n\u001b[1;32m    468\u001b[0m ):\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m--> 470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTD_HANDLED_FUNCTIONS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/_torch_func.py:568\u001b[0m, in \u001b[0;36m_stack\u001b[0;34m(list_of_tensordicts, dim, device, out, strict, contiguous, maybe_dense_stack)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    560\u001b[0m         _ErrorInteceptor(\n\u001b[1;32m    561\u001b[0m             key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to stack tensors on different devices at key\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m    565\u001b[0m     ):\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _stack(values, dim, maybe_dense_stack\u001b[38;5;241m=\u001b[39mmaybe_dense_stack)\n\u001b[0;32m--> 568\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    569\u001b[0m     key: stack_fn(key, values, is_not_init, is_tensor)\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, (values, is_not_init, is_tensor) \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    571\u001b[0m }\n\u001b[1;32m    573\u001b[0m result \u001b[38;5;241m=\u001b[39m TensorDict\u001b[38;5;241m.\u001b[39m_new_unsafe(\n\u001b[1;32m    574\u001b[0m     out,\n\u001b[1;32m    575\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mLazyStackedTensorDict\u001b[38;5;241m.\u001b[39m_compute_batch_size(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    579\u001b[0m )\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tc:\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/_torch_func.py:569\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    560\u001b[0m         _ErrorInteceptor(\n\u001b[1;32m    561\u001b[0m             key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to stack tensors on different devices at key\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m    565\u001b[0m     ):\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _stack(values, dim, maybe_dense_stack\u001b[38;5;241m=\u001b[39mmaybe_dense_stack)\n\u001b[1;32m    568\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 569\u001b[0m     key: \u001b[43mstack_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_not_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, (values, is_not_init, is_tensor) \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    571\u001b[0m }\n\u001b[1;32m    573\u001b[0m result \u001b[38;5;241m=\u001b[39m TensorDict\u001b[38;5;241m.\u001b[39m_new_unsafe(\n\u001b[1;32m    574\u001b[0m     out,\n\u001b[1;32m    575\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mLazyStackedTensorDict\u001b[38;5;241m.\u001b[39m_compute_batch_size(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    579\u001b[0m )\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tc:\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/_torch_func.py:566\u001b[0m, in \u001b[0;36m_stack.<locals>.stack_fn\u001b[0;34m(key, values, is_not_init, is_tensor)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(values, dim)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    560\u001b[0m     _ErrorInteceptor(\n\u001b[1;32m    561\u001b[0m         key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to stack tensors on different devices at key\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m    565\u001b[0m ):\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaybe_dense_stack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_dense_stack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/_torch_func.py:568\u001b[0m, in \u001b[0;36m_stack\u001b[0;34m(list_of_tensordicts, dim, device, out, strict, contiguous, maybe_dense_stack)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    560\u001b[0m         _ErrorInteceptor(\n\u001b[1;32m    561\u001b[0m             key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to stack tensors on different devices at key\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m    565\u001b[0m     ):\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _stack(values, dim, maybe_dense_stack\u001b[38;5;241m=\u001b[39mmaybe_dense_stack)\n\u001b[0;32m--> 568\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    569\u001b[0m     key: stack_fn(key, values, is_not_init, is_tensor)\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, (values, is_not_init, is_tensor) \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    571\u001b[0m }\n\u001b[1;32m    573\u001b[0m result \u001b[38;5;241m=\u001b[39m TensorDict\u001b[38;5;241m.\u001b[39m_new_unsafe(\n\u001b[1;32m    574\u001b[0m     out,\n\u001b[1;32m    575\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mLazyStackedTensorDict\u001b[38;5;241m.\u001b[39m_compute_batch_size(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    579\u001b[0m )\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tc:\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/_torch_func.py:569\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    560\u001b[0m         _ErrorInteceptor(\n\u001b[1;32m    561\u001b[0m             key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to stack tensors on different devices at key\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m    565\u001b[0m     ):\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _stack(values, dim, maybe_dense_stack\u001b[38;5;241m=\u001b[39mmaybe_dense_stack)\n\u001b[1;32m    568\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 569\u001b[0m     key: \u001b[43mstack_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_not_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, (values, is_not_init, is_tensor) \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    571\u001b[0m }\n\u001b[1;32m    573\u001b[0m result \u001b[38;5;241m=\u001b[39m TensorDict\u001b[38;5;241m.\u001b[39m_new_unsafe(\n\u001b[1;32m    574\u001b[0m     out,\n\u001b[1;32m    575\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mLazyStackedTensorDict\u001b[38;5;241m.\u001b[39m_compute_batch_size(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    579\u001b[0m )\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tc:\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/_torch_func.py:566\u001b[0m, in \u001b[0;36m_stack.<locals>.stack_fn\u001b[0;34m(key, values, is_not_init, is_tensor)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(values, dim)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    560\u001b[0m     _ErrorInteceptor(\n\u001b[1;32m    561\u001b[0m         key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to stack tensors on different devices at key\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m    565\u001b[0m ):\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaybe_dense_stack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_dense_stack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchrl-mlagents/lib/python3.10/site-packages/tensordict/_torch_func.py:462\u001b[0m, in \u001b[0;36m_stack\u001b[0;34m(list_of_tensordicts, dim, device, out, strict, contiguous, maybe_dense_stack)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _stack(list_of_tensordicts, dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sets of keys in the tensordicts to stack are exclusive. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using `LazyStackedTensorDict.maybe_dense_stack` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m             )\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(_tensordict, LazyStackedTensorDict)\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _tensordict \u001b[38;5;129;01min\u001b[39;00m list_of_tensordicts\n\u001b[1;32m    471\u001b[0m ):\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# Let's try to see if all tensors have the same shape\u001b[39;00m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# If so, we can assume that we can densly stack the sub-tds\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The sets of keys in the tensordicts to stack are exclusive. Consider using `LazyStackedTensorDict.maybe_dense_stack` instead."
     ]
    }
   ],
   "source": [
    "cur_frames = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "#env.reset()\n",
    "\n",
    "# Collect batches of data\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # Train on the collected batch\n",
    "    for _ in range(num_epochs):\n",
    "        advantage_module(\n",
    "            tensordict_data,\n",
    "            params=loss_module.critic_network_params,\n",
    "            target_params=loss_module.target_critic_network_params,\n",
    "        )\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "\n",
    "        # During each epoch, a batch of data is trained in sub-batches\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Back propogate and optimize\n",
    "            loss_value.backward()\n",
    "\n",
    "            # Bounding the grad is good practice\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(),\n",
    "                max_grad_norm\n",
    "            )\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    # That's all the training we need!\n",
    "    # But let's monitor how well our policy is working.\n",
    "\n",
    "    reward_sum = tensordict_data['next', 'reward'].sum().item()\n",
    "    max_step_count = tensordict_data[\"step_count\"].max().item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Execute a rollout with the trained policy\n",
    "        eval_rollout = env.rollout(100, policy_module)\n",
    "        eval_reward = eval_rollout[\"next\", \"reward\"]\n",
    "        eval_reward_sum = eval_reward.sum().item()\n",
    "        eval_reward_mean = eval_reward.mean().item()\n",
    "        eval_step_count = eval_rollout[\"step_count\"].max().item()\n",
    "\n",
    "        cur_frames += tensordict_data.numel()\n",
    "        run_time = str(datetime.now() - start_time).split('.')[0]\n",
    "\n",
    "        eval_str = (\n",
    "            f\"[{cur_frames}/{total_frames}\"\n",
    "            f\" {run_time}s]\"\n",
    "            f\" eval: (reward sum: {eval_reward_sum}\"\n",
    "            f\", reward mean: {eval_reward_mean}\"\n",
    "            f\", step count: {eval_step_count}\"\n",
    "            f\"), collection: (max step count: {max_step_count})\"\n",
    "        )\n",
    "        print(eval_str)\n",
    "\n",
    "    # This controls the learning rate scheduler, which is good practice\n",
    "    # to include\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that according to the [docs](https://www.gymlibrary.dev/environments/mujoco/inverted_pendulum/) for \"InvertedPendulum-v4\", the environnment automatically resets at step 1000, so 1000 is the maximum possible reward sum. This model is fairly well trained after just a few minutes, since we're consistently reaching the maximum reward when we evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl-mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
